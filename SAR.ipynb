{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zE2TKGeZ_F5Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split as sk_split\n",
        "import math\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "lBL39-epcnGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_USER_COL = \"userID\"\n",
        "DEFAULT_ITEM_COL = \"itemID\"\n",
        "DEFAULT_RATING_COL = \"rating\"\n",
        "DEFAULT_LABEL_COL = \"label\"\n",
        "DEFAULT_TITLE_COL = \"title\"\n",
        "DEFAULT_GENRE_COL = \"genre\"\n",
        "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
        "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
        "DEFAULT_PREDICTION_COL = \"prediction\"\n",
        "DEFAULT_SIMILARITY_COL = \"sim\"\n",
        "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
        "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\"\n",
        "\n",
        "DEFAULT_HEADER = (\n",
        "    DEFAULT_USER_COL,\n",
        "    DEFAULT_ITEM_COL,\n",
        "    DEFAULT_RATING_COL,\n",
        "    DEFAULT_TIMESTAMP_COL,\n",
        ")\n",
        "\n",
        "COL_DICT = {\n",
        "    \"col_user\": DEFAULT_USER_COL,\n",
        "    \"col_item\": DEFAULT_ITEM_COL,\n",
        "    \"col_rating\": DEFAULT_RATING_COL,\n",
        "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
        "}\n",
        "\n",
        "# Filtering variables\n",
        "DEFAULT_K = 10\n",
        "DEFAULT_THRESHOLD = 10\n",
        "\n",
        "# Other\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "FF9SNnDMc_IH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def is_jupyter():\n",
        "    \"\"\"Check if the module is running on Jupyter notebook/console.\n",
        "    Returns:\n",
        "        bool: True if the module is running on Jupyter notebook or Jupyter console,\n",
        "        False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        shell_name = get_ipython().__class__.__name__\n",
        "        if shell_name == \"ZMQInteractiveShell\":\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_databricks():\n",
        "    \"\"\"Check if the module is running on Databricks.\n",
        "    Returns:\n",
        "        bool: True if the module is running on Databricks notebook,\n",
        "        False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if os.path.realpath(\".\") == \"/databricks/driver\":\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except NameError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "TppmtJ7zrKcf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import requests\n",
        "import math\n",
        "import zipfile\n",
        "from contextlib import contextmanager\n",
        "from tempfile import TemporaryDirectory\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def maybe_download(url, filename=None, work_directory=\".\", expected_bytes=None):\n",
        "    \"\"\"Download a file if it is not already downloaded.\n",
        "    Args:\n",
        "        filename (str): File name.\n",
        "        work_directory (str): Working directory.\n",
        "        url (str): URL of the file to download.\n",
        "        expected_bytes (int): Expected file size in bytes.\n",
        "    Returns:\n",
        "        str: File path of the file downloaded.\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        filename = url.split(\"/\")[-1]\n",
        "    os.makedirs(work_directory, exist_ok=True)\n",
        "    filepath = os.path.join(work_directory, filename)\n",
        "    print(filepath)\n",
        "    if not os.path.exists(filepath):\n",
        "        r = requests.get(url, stream=True)\n",
        "        if r.status_code == 200:\n",
        "            log.info(f\"Downloading {url}\")\n",
        "            total_size = int(r.headers.get(\"content-length\", 0))\n",
        "            block_size = 1024\n",
        "            num_iterables = math.ceil(total_size / block_size)\n",
        "            with open(filepath, \"wb\") as file:\n",
        "                for data in tqdm(\n",
        "                    r.iter_content(block_size),\n",
        "                    total=num_iterables,\n",
        "                    unit=\"KB\",\n",
        "                    unit_scale=True,\n",
        "                ):\n",
        "                    file.write(data)\n",
        "        else:\n",
        "            log.error(f\"Problem downloading {url}\")\n",
        "            r.raise_for_status()\n",
        "    else:\n",
        "        log.info(f\"File {filepath} already downloaded\")\n",
        "        print(\"File Found\")\n",
        "    if expected_bytes is not None:\n",
        "        statinfo = os.stat(filepath)\n",
        "        if statinfo.st_size != expected_bytes:\n",
        "            os.remove(filepath)\n",
        "            raise IOError(f\"Failed to verify {filepath}\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def download_path(path=None):\n",
        "    \"\"\"Return a path to download data. If `path=None`, then it yields a temporal path that is eventually deleted,\n",
        "    otherwise the real path of the input.\n",
        "    Args:\n",
        "        path (str): Path to download data.\n",
        "    Returns:\n",
        "        str: Real path where the data is stored.\n",
        "    Examples:\n",
        "        >>> with download_path() as path:\n",
        "        >>> ... maybe_download(url=\"http://example.com/file.zip\", work_directory=path)\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        tmp_dir = TemporaryDirectory()\n",
        "        try:\n",
        "            yield tmp_dir.name\n",
        "        finally:\n",
        "            tmp_dir.cleanup()\n",
        "    else:\n",
        "        path = os.path.realpath(path)\n",
        "        yield path\n",
        "\n",
        "\n",
        "def unzip_file(zip_src, dst_dir, clean_zip_file=False):\n",
        "    \"\"\"Unzip a file\n",
        "    Args:\n",
        "        zip_src (str): Zip file.\n",
        "        dst_dir (str): Destination folder.\n",
        "        clean_zip_file (bool): Whether or not to clean the zip file.\n",
        "    \"\"\"\n",
        "    fz = zipfile.ZipFile(zip_src, \"r\")\n",
        "    for file in fz.namelist():\n",
        "        fz.extract(file, dst_dir)\n",
        "    if clean_zip_file:\n",
        "        os.remove(zip_src)"
      ],
      "metadata": {
        "id": "E0w_zE5XrXKa"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import shutil\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from zipfile import ZipFile\n",
        "\n",
        "try:\n",
        "    from pyspark.sql.types import (\n",
        "        StructType,\n",
        "        StructField,\n",
        "        StringType,\n",
        "        IntegerType,\n",
        "        FloatType,\n",
        "        LongType,\n",
        "    )\n",
        "except ImportError:\n",
        "    pass  # so the environment without spark doesn't break\n",
        "\n",
        "\n",
        "class _DataFormat:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sep,\n",
        "        path,\n",
        "        has_header=False,\n",
        "        item_sep=None,\n",
        "        item_path=None,\n",
        "        item_has_header=False,\n",
        "    ):\n",
        "        \"\"\"MovieLens data format container as a different size of MovieLens data file\n",
        "        has a different format\n",
        "        Args:\n",
        "            sep (str): Rating data delimiter\n",
        "            path (str): Rating data path within the original zip file\n",
        "            has_header (bool): Whether the rating data contains a header line or not\n",
        "            item_sep (str): Item data delimiter\n",
        "            item_path (str): Item data path within the original zip file\n",
        "            item_has_header (bool): Whether the item data contains a header line or not\n",
        "        \"\"\"\n",
        "\n",
        "        # Rating file\n",
        "        self._sep = sep\n",
        "        self._path = path\n",
        "        self._has_header = has_header\n",
        "\n",
        "        # Item file\n",
        "        self._item_sep = item_sep\n",
        "        self._item_path = item_path\n",
        "        self._item_has_header = item_has_header\n",
        "\n",
        "    @property\n",
        "    def separator(self):\n",
        "        return self._sep\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        return self._path\n",
        "\n",
        "    @property\n",
        "    def has_header(self):\n",
        "        return self._has_header\n",
        "\n",
        "    @property\n",
        "    def item_separator(self):\n",
        "        return self._item_sep\n",
        "\n",
        "    @property\n",
        "    def item_path(self):\n",
        "        return self._item_path\n",
        "\n",
        "    @property\n",
        "    def item_has_header(self):\n",
        "        return self._item_has_header\n",
        "\n",
        "\n",
        "# 10m and 20m data do not have user data\n",
        "DATA_FORMAT = {\n",
        "    \"100k\": _DataFormat(\"\\t\", \"ml-100k/u.data\", False, \"|\", \"ml-100k/u.item\", False),\n",
        "    \"1m\": _DataFormat(\n",
        "        \"::\", \"ml-1m/ratings.dat\", False, \"::\", \"ml-1m/movies.dat\", False\n",
        "    ),\n",
        "    \"10m\": _DataFormat(\n",
        "        \"::\", \"ml-10M100K/ratings.dat\", False, \"::\", \"ml-10M100K/movies.dat\", False\n",
        "    ),\n",
        "    \"20m\": _DataFormat(\",\", \"ml-20m/ratings.csv\", True, \",\", \"ml-20m/movies.csv\", True),\n",
        "}\n",
        "\n",
        "# Fake data for testing only\n",
        "MOCK_DATA_FORMAT = {\n",
        "    \"mock100\": {\"size\": 100, \"seed\": 6},\n",
        "}\n",
        "\n",
        "# 100K data genres index to string mapper. For 1m, 10m, and 20m, the genres labels are already in the dataset.\n",
        "GENRES = (\n",
        "    \"unknown\",\n",
        "    \"Action\",\n",
        "    \"Adventure\",\n",
        "    \"Animation\",\n",
        "    \"Children's\",\n",
        "    \"Comedy\",\n",
        "    \"Crime\",\n",
        "    \"Documentary\",\n",
        "    \"Drama\",\n",
        "    \"Fantasy\",\n",
        "    \"Film-Noir\",\n",
        "    \"Horror\",\n",
        "    \"Musical\",\n",
        "    \"Mystery\",\n",
        "    \"Romance\",\n",
        "    \"Sci-Fi\",\n",
        "    \"Thriller\",\n",
        "    \"War\",\n",
        "    \"Western\",\n",
        ")\n",
        "\n",
        "\n",
        "# Warning and error messages\n",
        "WARNING_MOVIE_LENS_HEADER = \"\"\"MovieLens rating dataset has four columns\n",
        "    (user id, movie id, rating, and timestamp), but more than four column names are provided.\n",
        "    Will only use the first four column names.\"\"\"\n",
        "WARNING_HAVE_SCHEMA_AND_HEADER = \"\"\"Both schema and header are provided.\n",
        "    The header argument will be ignored.\"\"\"\n",
        "ERROR_MOVIE_LENS_SIZE = (\n",
        "    \"Invalid data size. Should be one of {100k, 1m, 10m, or 20m, or mock100}\"\n",
        ")\n",
        "ERROR_HEADER = \"Header error. At least user and movie column names should be provided\"\n",
        "\n",
        "\n",
        "def load_pandas_df(\n",
        "    size=\"100k\",\n",
        "    header=None,\n",
        "    local_cache_path=None,\n",
        "    title_col=None,\n",
        "    genres_col=None,\n",
        "    year_col=None,\n",
        "):\n",
        "    \"\"\"Loads the MovieLens dataset as pd.DataFrame.\n",
        "    Download the dataset from https://files.grouplens.org/datasets/movielens, unzip, and load.\n",
        "    To load movie information only, you can use load_item_df function.\n",
        "    Args:\n",
        "        size (str): Size of the data to load. One of (\"100k\", \"1m\", \"10m\", \"20m\", \"mock100\").\n",
        "        header (list or tuple or None): Rating dataset header.\n",
        "            If `size` is set to any of 'MOCK_DATA_FORMAT', this parameter is ignored and data is rendered using the 'DEFAULT_HEADER' instead.\n",
        "        local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.\n",
        "            If None, all the intermediate files will be stored in a temporary directory and removed after use.\n",
        "            If `size` is set to any of 'MOCK_DATA_FORMAT', this parameter is ignored.\n",
        "        title_col (str): Movie title column name. If None, the column will not be loaded.\n",
        "        genres_col (str): Genres column name. Genres are '|' separated string.\n",
        "            If None, the column will not be loaded.\n",
        "        year_col (str): Movie release year column name. If None, the column will not be loaded.\n",
        "            If `size` is set to any of 'MOCK_DATA_FORMAT', this parameter is ignored.\n",
        "    Returns:\n",
        "        pandas.DataFrame: Movie rating dataset.\n",
        "    **Examples**\n",
        "    .. code-block:: python\n",
        "        # To load just user-id, item-id, and ratings from MovieLens-1M dataset,\n",
        "        df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating'))\n",
        "        # To load rating's timestamp together,\n",
        "        df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating', 'Timestamp'))\n",
        "        # To load movie's title, genres, and released year info along with the ratings data,\n",
        "        df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating', 'Timestamp'),\n",
        "            title_col='Title',\n",
        "            genres_col='Genres',\n",
        "            year_col='Year'\n",
        "        )\n",
        "    \"\"\"\n",
        "    size = size.lower()\n",
        "    if size not in DATA_FORMAT and size not in MOCK_DATA_FORMAT:\n",
        "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
        "\n",
        "    if header is None:\n",
        "        header = DEFAULT_HEADER\n",
        "    elif len(header) < 2:\n",
        "        raise ValueError(ERROR_HEADER)\n",
        "    elif len(header) > 4:\n",
        "        warnings.warn(WARNING_MOVIE_LENS_HEADER)\n",
        "        header = header[:4]\n",
        "\n",
        "    if size in MOCK_DATA_FORMAT:\n",
        "        # generate fake data\n",
        "        return MockMovielensSchema.get_df(\n",
        "            keep_first_n_cols=len(header),\n",
        "            keep_title_col=(title_col is not None),\n",
        "            keep_genre_col=(genres_col is not None),\n",
        "            **MOCK_DATA_FORMAT[\n",
        "                size\n",
        "            ],  # supply the rest of the kwarg with the dictionary\n",
        "        )\n",
        "\n",
        "    movie_col = header[1]\n",
        "\n",
        "    with download_path(local_cache_path) as path:\n",
        "        filepath = os.path.join(path, \"ml-{}.zip\".format(size))\n",
        "        #filepath = 'ml-latest-small.zip'\n",
        "        datapath, item_datapath = _maybe_download_and_extract(size, filepath)\n",
        "\n",
        "        # Load movie features such as title, genres, and release year\n",
        "        item_df = _load_item_df(\n",
        "            size, item_datapath, movie_col, title_col, genres_col, year_col\n",
        "        )\n",
        "\n",
        "        # Load rating data\n",
        "        df = pd.read_csv(\n",
        "            datapath,\n",
        "            sep=DATA_FORMAT[size].separator,\n",
        "            engine=\"python\",\n",
        "            names=header,\n",
        "            usecols=[*range(len(header))],\n",
        "            header=0 if DATA_FORMAT[size].has_header else None,\n",
        "        )\n",
        "\n",
        "        # Convert 'rating' type to float\n",
        "        if len(header) > 2:\n",
        "            df[header[2]] = df[header[2]].astype(float)\n",
        "\n",
        "        # Merge rating df w/ item_df\n",
        "        if item_df is not None:\n",
        "            df = df.merge(item_df, on=header[1])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_item_df(\n",
        "    size=\"100k\",\n",
        "    local_cache_path=None,\n",
        "    movie_col=DEFAULT_ITEM_COL,\n",
        "    title_col=None,\n",
        "    genres_col=None,\n",
        "    year_col=None,\n",
        "):\n",
        "    \"\"\"Loads Movie info.\n",
        "    Args:\n",
        "        size (str): Size of the data to load. One of (\"100k\", \"1m\", \"10m\", \"20m\").\n",
        "        local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.\n",
        "            If None, all the intermediate files will be stored in a temporary directory and removed after use.\n",
        "        movie_col (str): Movie id column name.\n",
        "        title_col (str): Movie title column name. If None, the column will not be loaded.\n",
        "        genres_col (str): Genres column name. Genres are '|' separated string.\n",
        "            If None, the column will not be loaded.\n",
        "        year_col (str): Movie release year column name. If None, the column will not be loaded.\n",
        "    Returns:\n",
        "        pandas.DataFrame: Movie information data, such as title, genres, and release year.\n",
        "    \"\"\"\n",
        "    size = size.lower()\n",
        "    if size not in DATA_FORMAT:\n",
        "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
        "\n",
        "    with download_path(local_cache_path) as path:\n",
        "        filepath = os.path.join(path, \"ml-{}.zip\".format(size))\n",
        "        _, item_datapath = _maybe_download_and_extract(size, filepath)\n",
        "        item_df = _load_item_df(\n",
        "            size, item_datapath, movie_col, title_col, genres_col, year_col\n",
        "        )\n",
        "\n",
        "    return item_df\n",
        "\n",
        "\n",
        "def _load_item_df(size, item_datapath, movie_col, title_col, genres_col, year_col):\n",
        "    \"\"\"Loads Movie info\"\"\"\n",
        "    if title_col is None and genres_col is None and year_col is None:\n",
        "        return None\n",
        "\n",
        "    item_header = [movie_col]\n",
        "    usecols = [0]\n",
        "\n",
        "    # Year is parsed from title\n",
        "    if title_col is not None or year_col is not None:\n",
        "        item_header.append(\"title_year\")\n",
        "        usecols.append(1)\n",
        "\n",
        "    genres_header_100k = None\n",
        "    if genres_col is not None:\n",
        "        # 100k data's movie genres are encoded as a binary array (the last 19 fields)\n",
        "        # For details, see https://files.grouplens.org/datasets/movielens/ml-100k-README.txt\n",
        "        if size == \"100k\":\n",
        "            genres_header_100k = [*(str(i) for i in range(19))]\n",
        "            item_header.extend(genres_header_100k)\n",
        "            usecols.extend([*range(5, 24)])  # genres columns\n",
        "        else:\n",
        "            item_header.append(genres_col)\n",
        "            usecols.append(2)  # genres column\n",
        "\n",
        "    item_df = pd.read_csv(\n",
        "        item_datapath,\n",
        "        sep=DATA_FORMAT[size].item_separator,\n",
        "        engine=\"python\",\n",
        "        names=item_header,\n",
        "        usecols=usecols,\n",
        "        header=0 if DATA_FORMAT[size].item_has_header else None,\n",
        "        encoding=\"ISO-8859-1\",\n",
        "    )\n",
        "\n",
        "    # Convert 100k data's format: '0|0|1|...' to 'Action|Romance|...\"\n",
        "    if genres_header_100k is not None:\n",
        "        item_df[genres_col] = item_df[genres_header_100k].values.tolist()\n",
        "        item_df[genres_col] = item_df[genres_col].map(\n",
        "            lambda l: \"|\".join([GENRES[i] for i, v in enumerate(l) if v == 1])\n",
        "        )\n",
        "\n",
        "        item_df.drop(genres_header_100k, axis=1, inplace=True)\n",
        "\n",
        "    # Parse year from movie title. Note, MovieLens title format is \"title (year)\"\n",
        "    # Note, there are very few records that are missing the year info.\n",
        "    if year_col is not None:\n",
        "\n",
        "        def parse_year(t):\n",
        "            parsed = re.split(\"[()]\", t)\n",
        "            if len(parsed) > 2 and parsed[-2].isdecimal():\n",
        "                return parsed[-2]\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        item_df[year_col] = item_df[\"title_year\"].map(parse_year)\n",
        "        if title_col is None:\n",
        "            item_df.drop(\"title_year\", axis=1, inplace=True)\n",
        "\n",
        "    if title_col is not None:\n",
        "        item_df.rename(columns={\"title_year\": title_col}, inplace=True)\n",
        "\n",
        "    return item_df\n",
        "\n",
        "\n",
        "def _maybe_download_and_extract(size, dest_path):\n",
        "    \"\"\"Downloads and extracts MovieLens rating and item datafiles if they donâ€™t already exist\"\"\"\n",
        "    dirs, _ = os.path.split(dest_path)\n",
        "    if not os.path.exists(dirs):\n",
        "        os.makedirs(dirs)\n",
        "\n",
        "    _, rating_filename = os.path.split(DATA_FORMAT[size].path)\n",
        "    rating_path = os.path.join(dirs, rating_filename)\n",
        "    _, item_filename = os.path.split(DATA_FORMAT[size].item_path)\n",
        "    item_path = os.path.join(dirs, item_filename)\n",
        "\n",
        "    if not os.path.exists(rating_path) or not os.path.exists(item_path):\n",
        "        download_movielens(size, dest_path)\n",
        "        extract_movielens(size, rating_path, item_path, dest_path)\n",
        "\n",
        "    return rating_path, item_path\n",
        "\n",
        "\n",
        "def download_movielens(size, dest_path):\n",
        "    \"\"\"Downloads MovieLens datafile.\n",
        "    Args:\n",
        "        size (str): Size of the data to load. One of (\"100k\", \"1m\", \"10m\", \"20m\").\n",
        "        dest_path (str): File path for the downloaded file\n",
        "    \"\"\"\n",
        "    if size not in DATA_FORMAT:\n",
        "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
        "\n",
        "    url = \"https://files.grouplens.org/datasets/movielens/ml-\" + size + \".zip\"\n",
        "    dirs, file = os.path.split(dest_path)\n",
        "    maybe_download(url, file, work_directory=dirs)\n",
        "\n",
        "\n",
        "def extract_movielens(size, rating_path, item_path, zip_path):\n",
        "    \"\"\"Extract MovieLens rating and item datafiles from the MovieLens raw zip file.\n",
        "    To extract all files instead of just rating and item datafiles,\n",
        "    use ZipFile's extractall(path) instead.\n",
        "    Args:\n",
        "        size (str): Size of the data to load. One of (\"100k\", \"1m\", \"10m\", \"20m\").\n",
        "        rating_path (str): Destination path for rating datafile\n",
        "        item_path (str): Destination path for item datafile\n",
        "        zip_path (str): zipfile path\n",
        "    \"\"\"\n",
        "    with ZipFile(zip_path, \"r\") as z:\n",
        "        with z.open(DATA_FORMAT[size].path) as zf, open(rating_path, \"wb\") as f:\n",
        "            shutil.copyfileobj(zf, f)\n",
        "        with z.open(DATA_FORMAT[size].item_path) as zf, open(item_path, \"wb\") as f:\n",
        "            shutil.copyfileobj(zf, f)\n",
        "\n",
        "def unique_columns(df, *, columns):\n",
        "    return not df[columns].duplicated().any()"
      ],
      "metadata": {
        "id": "e9Ae__iItWnN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def exponential_decay(value, max_val, half_life):\n",
        "    \"\"\"Compute decay factor for a given value based on an exponential decay.\n",
        "    Values greater than `max_val` will be set to 1.\n",
        "    Args:\n",
        "        value (numeric): Value to calculate decay factor\n",
        "        max_val (numeric): Value at which decay factor will be 1\n",
        "        half_life (numeric): Value at which decay factor will be 0.5\n",
        "    Returns:\n",
        "        float: Decay factor\n",
        "    \"\"\"\n",
        "    return np.minimum(1.0, np.power(0.5, (max_val - value) / half_life))\n",
        "\n",
        "\n",
        "def jaccard(cooccurrence):\n",
        "    \"\"\"Helper method to calculate the Jaccard similarity of a matrix of co-occurrences.\n",
        "    When comparing Jaccard with count co-occurrence and lift similarity, count favours\n",
        "    predictability, meaning that the most popular items will be recommended most of\n",
        "    the time. Lift, by contrast, favours discoverability/serendipity, meaning that an\n",
        "    item that is less popular overall but highly favoured by a small subset of users\n",
        "    is more likely to be recommended. Jaccard is a compromise between the two.\n",
        "    Args:\n",
        "        cooccurrence (numpy.ndarray): the symmetric matrix of co-occurrences of items.\n",
        "    Returns:\n",
        "        numpy.ndarray: The matrix of Jaccard similarities between any two items.\n",
        "    \"\"\"\n",
        "\n",
        "    diag = cooccurrence.diagonal()\n",
        "    diag_rows = np.expand_dims(diag, axis=0)\n",
        "    diag_cols = np.expand_dims(diag, axis=1)\n",
        "\n",
        "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "        result = cooccurrence / (diag_rows + diag_cols - cooccurrence)\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "\n",
        "def lift(cooccurrence):\n",
        "    \"\"\"Helper method to calculate the Lift of a matrix of co-occurrences. In comparison\n",
        "    with basic co-occurrence and Jaccard similarity, lift favours discoverability and\n",
        "    serendipity, as opposed to co-occurrence that favours the most popular items, and\n",
        "    Jaccard that is a compromise between the two.\n",
        "    Args:\n",
        "        cooccurrence (numpy.ndarray): The symmetric matrix of co-occurrences of items.\n",
        "    Returns:\n",
        "        numpy.ndarray: The matrix of Lifts between any two items.\n",
        "    \"\"\"\n",
        "\n",
        "    diag = cooccurrence.diagonal()\n",
        "    diag_rows = np.expand_dims(diag, axis=0)\n",
        "    diag_cols = np.expand_dims(diag, axis=1)\n",
        "\n",
        "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "        result = cooccurrence / (diag_rows * diag_cols)\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "\n",
        "def get_top_k_scored_items(scores, top_k, sort_top_k=False):\n",
        "    \"\"\"Extract top K items from a matrix of scores for each user-item pair, optionally sort results per user.\n",
        "    Args:\n",
        "        scores (numpy.ndarray): Score matrix (users x items).\n",
        "        top_k (int): Number of top items to recommend.\n",
        "        sort_top_k (bool): Flag to sort top k results.\n",
        "    Returns:\n",
        "        numpy.ndarray, numpy.ndarray:\n",
        "        - Indices into score matrix for each users top items.\n",
        "        - Scores corresponding to top items.\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure we're working with a dense ndarray\n",
        "    if isinstance(scores, sparse.spmatrix):\n",
        "        scores = scores.todense()\n",
        "\n",
        "    if scores.shape[1] < top_k:\n",
        "        logger.warning(\n",
        "            \"Number of items is less than top_k, limiting top_k to number of items\"\n",
        "        )\n",
        "    k = min(top_k, scores.shape[1])\n",
        "\n",
        "    test_user_idx = np.arange(scores.shape[0])[:, None]\n",
        "\n",
        "    # get top K items and scores\n",
        "    # this determines the un-ordered top-k item indices for each user\n",
        "    top_items = np.argpartition(scores, -k, axis=1)[:, -k:]\n",
        "    top_scores = scores[test_user_idx, top_items]\n",
        "\n",
        "    if sort_top_k:\n",
        "        sort_ind = np.argsort(-top_scores)\n",
        "        top_items = top_items[test_user_idx, sort_ind]\n",
        "        top_scores = top_scores[test_user_idx, sort_ind]\n",
        "\n",
        "    return np.array(top_items), np.array(top_scores)\n",
        "\n",
        "\n",
        "def binarize(a, threshold):\n",
        "    \"\"\"Binarize the values.\n",
        "    Args:\n",
        "        a (numpy.ndarray): Input array that needs to be binarized.\n",
        "        threshold (float): Threshold below which all values are set to 0, else 1.\n",
        "    Returns:\n",
        "        numpy.ndarray: Binarized array.\n",
        "    \"\"\"\n",
        "    return np.where(a > threshold, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def rescale(data, new_min=0, new_max=1, data_min=None, data_max=None):\n",
        "    \"\"\"Rescale/normalize the data to be within the range `[new_min, new_max]`\n",
        "    If data_min and data_max are explicitly provided, they will be used\n",
        "    as the old min/max values instead of taken from the data.\n",
        "    .. note::\n",
        "        This is same as the `scipy.MinMaxScaler` with the exception that we can override\n",
        "        the min/max of the old scale.\n",
        "    Args:\n",
        "        data (numpy.ndarray): 1d scores vector or 2d score matrix (users x items).\n",
        "        new_min (int|float): The minimum of the newly scaled data.\n",
        "        new_max (int|float): The maximum of the newly scaled data.\n",
        "        data_min (None|number): The minimum of the passed data [if omitted it will be inferred].\n",
        "        data_max (None|number): The maximum of the passed data [if omitted it will be inferred].\n",
        "    Returns:\n",
        "        numpy.ndarray: The newly scaled/normalized data.\n",
        "    \"\"\"\n",
        "    data_min = data.min() if data_min is None else data_min\n",
        "    data_max = data.max() if data_max is None else data_max\n",
        "    return (data - data_min) / (data_max - data_min) * (new_max - new_min) + new_min"
      ],
      "metadata": {
        "id": "8ZG1UV1-oED9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import lru_cache, wraps\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def user_item_pairs(\n",
        "    user_df,\n",
        "    item_df,\n",
        "    user_col=DEFAULT_USER_COL,\n",
        "    item_col=DEFAULT_ITEM_COL,\n",
        "    user_item_filter_df=None,\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "):\n",
        "    \"\"\"Get all pairs of users and items data.\n",
        "    Args:\n",
        "        user_df (pandas.DataFrame): User data containing unique user ids and maybe their features.\n",
        "        item_df (pandas.DataFrame): Item data containing unique item ids and maybe their features.\n",
        "        user_col (str): User id column name.\n",
        "        item_col (str): Item id column name.\n",
        "        user_item_filter_df (pd.DataFrame): User-item pairs to be used as a filter.\n",
        "        shuffle (bool): If True, shuffles the result.\n",
        "        seed (int): Random seed for shuffle\n",
        "    Returns:\n",
        "        pandas.DataFrame: All pairs of user-item from user_df and item_df, excepting the pairs in user_item_filter_df.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all user-item pairs\n",
        "    user_df[\"key\"] = 1\n",
        "    item_df[\"key\"] = 1\n",
        "    users_items = user_df.merge(item_df, on=\"key\")\n",
        "\n",
        "    user_df.drop(\"key\", axis=1, inplace=True)\n",
        "    item_df.drop(\"key\", axis=1, inplace=True)\n",
        "    users_items.drop(\"key\", axis=1, inplace=True)\n",
        "\n",
        "    # Filter\n",
        "    if user_item_filter_df is not None:\n",
        "        users_items = filter_by(users_items, user_item_filter_df, [user_col, item_col])\n",
        "\n",
        "    if shuffle:\n",
        "        users_items = users_items.sample(frac=1, random_state=seed).reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "\n",
        "    return users_items\n",
        "\n",
        "\n",
        "def filter_by(df, filter_by_df, filter_by_cols):\n",
        "    \"\"\"From the input DataFrame `df`, remove the records whose target column `filter_by_cols` values are\n",
        "    exist in the filter-by DataFrame `filter_by_df`.\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Source dataframe.\n",
        "        filter_by_df (pandas.DataFrame): Filter dataframe.\n",
        "        filter_by_cols (iterable of str): Filter columns.\n",
        "    Returns:\n",
        "        pandas.DataFrame: Dataframe filtered by `filter_by_df` on `filter_by_cols`.\n",
        "    \"\"\"\n",
        "\n",
        "    return df.loc[\n",
        "        ~df.set_index(filter_by_cols).index.isin(\n",
        "            filter_by_df.set_index(filter_by_cols).index\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "class LibffmConverter:\n",
        "    \"\"\"Converts an input dataframe to another dataframe in libffm format. A text file of the converted\n",
        "    Dataframe is optionally generated.\n",
        "    .. note::\n",
        "        The input dataframe is expected to represent the feature data in the following schema:\n",
        "        .. code-block:: python\n",
        "            |field-1|field-2|...|field-n|rating|\n",
        "            |feature-1-1|feature-2-1|...|feature-n-1|1|\n",
        "            |feature-1-2|feature-2-2|...|feature-n-2|0|\n",
        "            ...\n",
        "            |feature-1-i|feature-2-j|...|feature-n-k|0|\n",
        "        Where\n",
        "        1. each `field-*` is the column name of the dataframe (column of label/rating is excluded), and\n",
        "        2. `feature-*-*` can be either a string or a numerical value, representing the categorical variable or\n",
        "        actual numerical variable of the feature value in the field, respectively.\n",
        "        3. If there are ordinal variables represented in int types, users should make sure these columns\n",
        "        are properly converted to string type.\n",
        "        The above data will be converted to the libffm format by following the convention as explained in\n",
        "        `this paper <https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf>`_.\n",
        "        i.e. `<field_index>:<field_feature_index>:1` or `<field_index>:<field_feature_index>:<field_feature_value>`,\n",
        "        depending on the data type of the features in the original dataframe.\n",
        "    Args:\n",
        "        filepath (str): path to save the converted data.\n",
        "    Attributes:\n",
        "        field_count (int): count of field in the libffm format data\n",
        "        feature_count (int): count of feature in the libffm format data\n",
        "        filepath (str or None): file path where the output is stored - it can be None or a string\n",
        "    Examples:\n",
        "        >>> import pandas as pd\n",
        "        >>> df_feature = pd.DataFrame({\n",
        "                'rating': [1, 0, 0, 1, 1],\n",
        "                'field1': ['xxx1', 'xxx2', 'xxx4', 'xxx4', 'xxx4'],\n",
        "                'field2': [3, 4, 5, 6, 7],\n",
        "                'field3': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
        "                'field4': ['1', '2', '3', '4', '5']\n",
        "            })\n",
        "        >>> converter = LibffmConverter().fit(df_feature, col_rating='rating')\n",
        "        >>> df_out = converter.transform(df_feature)\n",
        "        >>> df_out\n",
        "            rating field1 field2   field3 field4\n",
        "        0       1  1:1:1  2:4:3  3:5:1.0  4:6:1\n",
        "        1       0  1:2:1  2:4:4  3:5:2.0  4:7:1\n",
        "        2       0  1:3:1  2:4:5  3:5:3.0  4:8:1\n",
        "        3       1  1:3:1  2:4:6  3:5:4.0  4:9:1\n",
        "        4       1  1:3:1  2:4:7  3:5:5.0  4:10:1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath=None):\n",
        "        self.filepath = filepath\n",
        "        self.col_rating = None\n",
        "        self.field_names = None\n",
        "        self.field_count = None\n",
        "        self.feature_count = None\n",
        "\n",
        "    def fit(self, df, col_rating=DEFAULT_RATING_COL):\n",
        "        \"\"\"Fit the dataframe for libffm format.\n",
        "        This method does nothing but check the validity of the input columns\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "            col_rating (str): rating of the data.\n",
        "        Return:\n",
        "            object: the instance of the converter\n",
        "        \"\"\"\n",
        "\n",
        "        # Check column types.\n",
        "        types = df.dtypes\n",
        "        if not all(\n",
        "            [\n",
        "                x == object or np.issubdtype(x, np.integer) or x == np.float\n",
        "                for x in types\n",
        "            ]\n",
        "        ):\n",
        "            raise TypeError(\"Input columns should be only object and/or numeric types.\")\n",
        "\n",
        "        if col_rating not in df.columns:\n",
        "            raise TypeError(\n",
        "                \"Column of {} is not in input dataframe columns\".format(col_rating)\n",
        "            )\n",
        "\n",
        "        self.col_rating = col_rating\n",
        "        self.field_names = list(df.drop(col_rating, axis=1).columns)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Tranform an input dataset with the same schema (column names and dtypes) to libffm format\n",
        "        by using the fitted converter.\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "        Return:\n",
        "            pandas.DataFrame: Output libffm format dataframe.\n",
        "        \"\"\"\n",
        "        if self.col_rating not in df.columns:\n",
        "            raise ValueError(\n",
        "                \"Input dataset does not contain the label column {} in the fitting dataset\".format(\n",
        "                    self.col_rating\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if not all([x in df.columns for x in self.field_names]):\n",
        "            raise ValueError(\n",
        "                \"Not all columns in the input dataset appear in the fitting dataset\"\n",
        "            )\n",
        "\n",
        "        # Encode field-feature.\n",
        "        idx = 1\n",
        "        self.field_feature_dict = {}\n",
        "        for field in self.field_names:\n",
        "            for feature in df[field].values:\n",
        "                # Check whether (field, feature) tuple exists in the dict or not.\n",
        "                # If not, put them into the key-values of the dict and count the index.\n",
        "                if (field, feature) not in self.field_feature_dict:\n",
        "                    self.field_feature_dict[(field, feature)] = idx\n",
        "                    if df[field].dtype == object:\n",
        "                        idx += 1\n",
        "            if df[field].dtype != object:\n",
        "                idx += 1\n",
        "\n",
        "        self.field_count = len(self.field_names)\n",
        "        self.feature_count = idx - 1\n",
        "\n",
        "        def _convert(field, feature, field_index, field_feature_index_dict):\n",
        "            field_feature_index = field_feature_index_dict[(field, feature)]\n",
        "            if isinstance(feature, str):\n",
        "                feature = 1\n",
        "            return \"{}:{}:{}\".format(field_index, field_feature_index, feature)\n",
        "\n",
        "        for col_index, col in enumerate(self.field_names):\n",
        "            df[col] = df[col].apply(\n",
        "                lambda x: _convert(col, x, col_index + 1, self.field_feature_dict)\n",
        "            )\n",
        "\n",
        "        # Move rating column to the first.\n",
        "        column_names = self.field_names[:]\n",
        "        column_names.insert(0, self.col_rating)\n",
        "        df = df[column_names]\n",
        "\n",
        "        if self.filepath is not None:\n",
        "            np.savetxt(self.filepath, df.values, delimiter=\" \", fmt=\"%s\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, df, col_rating=DEFAULT_RATING_COL):\n",
        "        \"\"\"Do fit and transform in a row\n",
        "        Args:\n",
        "            df (pandas.DataFrame): input Pandas dataframe.\n",
        "            col_rating (str): rating of the data.\n",
        "        Return:\n",
        "            pandas.DataFrame: Output libffm format dataframe.\n",
        "        \"\"\"\n",
        "        return self.fit(df, col_rating=col_rating).transform(df)\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"Get parameters (attributes) of the libffm converter\n",
        "        Return:\n",
        "            dict: A dictionary that contains parameters field count, feature count, and file path.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"field count\": self.field_count,\n",
        "            \"feature count\": self.feature_count,\n",
        "            \"file path\": self.filepath,\n",
        "        }\n",
        "\n",
        "\n",
        "def negative_feedback_sampler(\n",
        "    df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_label=DEFAULT_LABEL_COL,\n",
        "    col_feedback=\"feedback\",\n",
        "    ratio_neg_per_user=1,\n",
        "    pos_value=1,\n",
        "    neg_value=0,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Utility function to sample negative feedback from user-item interaction dataset.\n",
        "    This negative sampling function will take the user-item interaction data to create\n",
        "    binarized feedback, i.e., 1 and 0 indicate positive and negative feedback,\n",
        "    respectively.\n",
        "    Negative sampling is used in the literature frequently to generate negative samples\n",
        "    from a user-item interaction data.\n",
        "    See for example the `neural collaborative filtering paper <https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf>`_.\n",
        "    Args:\n",
        "        df (pandas.DataFrame): input data that contains user-item tuples.\n",
        "        col_user (str): user id column name.\n",
        "        col_item (str): item id column name.\n",
        "        col_label (str): label column name in df.\n",
        "        col_feedback (str): feedback column name in the returned data frame; it is used for the generated column\n",
        "            of positive and negative feedback.\n",
        "        ratio_neg_per_user (int): ratio of negative feedback w.r.t to the number of positive feedback for each user.\n",
        "            If the samples exceed the number of total possible negative feedback samples, it will be reduced to the\n",
        "            number of all the possible samples.\n",
        "        pos_value (float): value of positive feedback.\n",
        "        neg_value (float): value of negative feedback.\n",
        "        inplace (bool):\n",
        "        seed (int): seed for the random state of the sampling function.\n",
        "    Returns:\n",
        "        pandas.DataFrame: Data with negative feedback.\n",
        "    Examples:\n",
        "        >>> import pandas as pd\n",
        "        >>> df = pd.DataFrame({\n",
        "            'userID': [1, 2, 3],\n",
        "            'itemID': [1, 2, 3],\n",
        "            'rating': [5, 5, 5]\n",
        "        })\n",
        "        >>> df_neg_sampled = negative_feedback_sampler(\n",
        "            df, col_user='userID', col_item='itemID', ratio_neg_per_user=1\n",
        "        )\n",
        "        >>> df_neg_sampled\n",
        "        userID  itemID  feedback\n",
        "        1   1   1\n",
        "        1   2   0\n",
        "        2   2   1\n",
        "        2   1   0\n",
        "        3   3   1\n",
        "        3   1   0\n",
        "    \"\"\"\n",
        "    # Get all of the users and items.\n",
        "    items = df[col_item].unique()\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "    def sample_items(user_df):\n",
        "        # Sample negative items for the data frame restricted to a specific user\n",
        "        n_u = len(user_df)\n",
        "        neg_sample_size = max(round(n_u * ratio_neg_per_user), 1)\n",
        "        # Draw (n_u + neg_sample_size) items and keep neg_sample_size of these\n",
        "        # that are not already in user_df. This requires a set difference from items_sample\n",
        "        # instead of items, which is more efficient when len(items) is large.\n",
        "        sample_size = min(n_u + neg_sample_size, len(items))\n",
        "        items_sample = rng.choice(items, sample_size, replace=False)\n",
        "        new_items = np.setdiff1d(items_sample, user_df[col_item])[:neg_sample_size]\n",
        "        new_df = pd.DataFrame(\n",
        "            data={\n",
        "                col_user: user_df.name,\n",
        "                col_item: new_items,\n",
        "                col_label: neg_value,\n",
        "            }\n",
        "        )\n",
        "        return pd.concat([user_df, new_df], ignore_index=True)\n",
        "\n",
        "    res_df = df.copy()\n",
        "    res_df[col_label] = pos_value\n",
        "    return (\n",
        "        res_df.groupby(col_user)\n",
        "        .apply(sample_items)\n",
        "        .reset_index(drop=True)\n",
        "        .rename(columns={col_label: col_feedback})\n",
        "    )\n",
        "\n",
        "\n",
        "def has_columns(df, columns):\n",
        "    \"\"\"Check if DataFrame has necessary columns\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame\n",
        "        columns (list(str): columns to check for\n",
        "    Returns:\n",
        "        bool: True if DataFrame has specified columns.\n",
        "    \"\"\"\n",
        "\n",
        "    result = True\n",
        "    for column in columns:\n",
        "        if column not in df.columns:\n",
        "            logger.error(\"Missing column: {} in DataFrame\".format(column))\n",
        "            result = False\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def has_same_base_dtype(df_1, df_2, columns=None):\n",
        "    \"\"\"Check if specified columns have the same base dtypes across both DataFrames\n",
        "    Args:\n",
        "        df_1 (pandas.DataFrame): first DataFrame\n",
        "        df_2 (pandas.DataFrame): second DataFrame\n",
        "        columns (list(str)): columns to check, None checks all columns\n",
        "    Returns:\n",
        "        bool: True if DataFrames columns have the same base dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    if columns is None:\n",
        "        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):\n",
        "            logger.error(\n",
        "                \"Cannot test all columns because they are not all shared across DataFrames\"\n",
        "            )\n",
        "            return False\n",
        "        columns = df_1.columns\n",
        "\n",
        "    if not (\n",
        "        has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    result = True\n",
        "    for column in columns:\n",
        "        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:\n",
        "            logger.error(\"Columns {} do not have the same base datatype\".format(column))\n",
        "            result = False\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "class PandasHash:\n",
        "    \"\"\"Wrapper class to allow pandas objects (DataFrames or Series) to be hashable\"\"\"\n",
        "\n",
        "    # reserve space just for a single pandas object\n",
        "    __slots__ = \"pandas_object\"\n",
        "\n",
        "    def __init__(self, pandas_object):\n",
        "        \"\"\"Initialize class\n",
        "        Args:\n",
        "            pandas_object (pandas.DataFrame|pandas.Series): pandas object\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(pandas_object, (pd.DataFrame, pd.Series)):\n",
        "            raise TypeError(\"Can only wrap pandas DataFrame or Series objects\")\n",
        "        self.pandas_object = pandas_object\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"Overwrite equality comparison\n",
        "        Args:\n",
        "            other (pandas.DataFrame|pandas.Series): pandas object to compare\n",
        "        Returns:\n",
        "            bool: whether other object is the same as this one\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(self) == hash(other)\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"Overwrite hash operator for use with pandas objects\n",
        "        Returns:\n",
        "            int: hashed value of object\n",
        "        \"\"\"\n",
        "\n",
        "        hashable = tuple(self.pandas_object.values.tobytes())\n",
        "        if isinstance(self.pandas_object, pd.DataFrame):\n",
        "            hashable += tuple(self.pandas_object.columns)\n",
        "        else:\n",
        "            hashable += tuple(self.pandas_object.name)\n",
        "        return hash(hashable)\n",
        "\n",
        "\n",
        "def lru_cache_df(maxsize, typed=False):\n",
        "    \"\"\"Least-recently-used cache decorator for pandas Dataframes.\n",
        "    Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can\n",
        "    save time when an expensive or I/O bound function is periodically called with the same arguments.\n",
        "    Inspired in the `lru_cache function <https://docs.python.org/3/library/functools.html#functools.lru_cache>`_.\n",
        "    Args:\n",
        "        maxsize (int|None): max size of cache, if set to None cache is boundless\n",
        "        typed (bool): arguments of different types are cached separately\n",
        "    \"\"\"\n",
        "\n",
        "    def to_pandas_hash(val):\n",
        "        \"\"\"Return PandaHash object if input is a DataFrame otherwise return input unchanged\"\"\"\n",
        "        return PandasHash(val) if isinstance(val, pd.DataFrame) else val\n",
        "\n",
        "    def from_pandas_hash(val):\n",
        "        \"\"\"Extract DataFrame if input is PandaHash object otherwise return input unchanged\"\"\"\n",
        "        return val.pandas_object if isinstance(val, PandasHash) else val\n",
        "\n",
        "    def decorating_function(user_function):\n",
        "        @wraps(user_function)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            # convert DataFrames in args and kwargs to PandaHash objects\n",
        "            args = tuple([to_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: to_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return cached_wrapper(*args, **kwargs)\n",
        "\n",
        "        @lru_cache(maxsize=maxsize, typed=typed)\n",
        "        def cached_wrapper(*args, **kwargs):\n",
        "            # get DataFrames from PandaHash objects in args and kwargs\n",
        "            args = tuple([from_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: from_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return user_function(*args, **kwargs)\n",
        "\n",
        "        # retain lru_cache attributes\n",
        "        wrapper.cache_info = cached_wrapper.cache_info\n",
        "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorating_function"
      ],
      "metadata": {
        "id": "LuZYkkrYcoua"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    from pyspark.sql import functions as F, Window\n",
        "except ImportError:\n",
        "    pass  # so the environment without spark doesn't break\n",
        "\n",
        "\n",
        "def process_split_ratio(ratio):\n",
        "    \"\"\"Generate split ratio lists.\n",
        "    Args:\n",
        "        ratio (float or list): a float number that indicates split ratio or a list of float\n",
        "        numbers that indicate split ratios (if it is a multi-split).\n",
        "    Returns:\n",
        "        tuple:\n",
        "        - bool: A boolean variable multi that indicates if the splitting is multi or single.\n",
        "        - list: A list of normalized split ratios.\n",
        "    \"\"\"\n",
        "    if isinstance(ratio, float):\n",
        "        if ratio <= 0 or ratio >= 1:\n",
        "            raise ValueError(\"Split ratio has to be between 0 and 1\")\n",
        "\n",
        "        multi = False\n",
        "    elif isinstance(ratio, list):\n",
        "        if any([x <= 0 for x in ratio]):\n",
        "            raise ValueError(\n",
        "                \"All split ratios in the ratio list should be larger than 0.\"\n",
        "            )\n",
        "\n",
        "        # normalize split ratios if they are not summed to 1\n",
        "        if math.fsum(ratio) != 1.0:\n",
        "            ratio = [x / math.fsum(ratio) for x in ratio]\n",
        "\n",
        "        multi = True\n",
        "    else:\n",
        "        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n",
        "\n",
        "    return multi, ratio\n",
        "\n",
        "\n",
        "def min_rating_filter_pandas(\n",
        "    data,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Filter rating DataFrame for each user with minimum rating.\n",
        "    Filter rating data frame with minimum number of ratings for user/item is usually useful to\n",
        "    generate a new data frame with warm user/item. The warmth is defined by min_rating argument. For\n",
        "    example, a user is called warm if he has rated at least 4 items.\n",
        "    Args:\n",
        "        data (pandas.DataFrame): DataFrame of user-item tuples. Columns of user and item\n",
        "            should be present in the DataFrame while other columns like rating,\n",
        "            timestamp, etc. can be optional.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to\n",
        "            filter with min_rating.\n",
        "        col_user (str): column name of user ID.\n",
        "        col_item (str): column name of item ID.\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with at least columns of user and item that has been filtered by the given specifications.\n",
        "    \"\"\"\n",
        "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    return data.groupby(split_by_column).filter(lambda x: len(x) >= min_rating)\n",
        "\n",
        "\n",
        "def min_rating_filter_spark(\n",
        "    data,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Filter rating DataFrame for each user with minimum rating.\n",
        "    Filter rating data frame with minimum number of ratings for user/item is usually useful to\n",
        "    generate a new data frame with warm user/item. The warmth is defined by min_rating argument. For\n",
        "    example, a user is called warm if he has rated at least 4 items.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): DataFrame of user-item tuples. Columns of user and item\n",
        "            should be present in the DataFrame while other columns like rating,\n",
        "            timestamp, etc. can be optional.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to\n",
        "            filter with min_rating.\n",
        "        col_user (str): column name of user ID.\n",
        "        col_item (str): column name of item ID.\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: DataFrame with at least columns of user and item that has been filtered by the given specifications.\n",
        "    \"\"\"\n",
        "\n",
        "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    if min_rating > 1:\n",
        "        window = Window.partitionBy(split_by_column)\n",
        "        data = (\n",
        "            data.withColumn(\"_count\", F.count(split_by_column).over(window))\n",
        "            .where(F.col(\"_count\") >= min_rating)\n",
        "            .drop(\"_count\")\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def _get_column_name(name, col_user, col_item):\n",
        "    if name == \"user\":\n",
        "        return col_user\n",
        "    elif name == \"item\":\n",
        "        return col_item\n",
        "    else:\n",
        "        raise ValueError(\"name should be either 'user' or 'item'.\")\n",
        "\n",
        "\n",
        "def split_pandas_data_with_ratios(data, ratios, seed=42, shuffle=False):\n",
        "    \"\"\"Helper function to split pandas DataFrame with given ratios\n",
        "    .. note::\n",
        "        Implementation referenced from `this source <https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test>`_.\n",
        "    Args:\n",
        "        data (pandas.DataFrame): Pandas data frame to be split.\n",
        "        ratios (list of floats): list of ratios for split. The ratios have to sum to 1.\n",
        "        seed (int): random seed.\n",
        "        shuffle (bool): whether data will be shuffled when being split.\n",
        "    Returns:\n",
        "        list: List of pd.DataFrame split by the given specifications.\n",
        "    \"\"\"\n",
        "    if math.fsum(ratios) != 1.0:\n",
        "        raise ValueError(\"The ratios have to sum to 1\")\n",
        "\n",
        "    split_index = np.cumsum(ratios).tolist()[:-1]\n",
        "\n",
        "    if shuffle:\n",
        "        data = data.sample(frac=1, random_state=seed)\n",
        "\n",
        "    splits = np.split(data, [round(x * len(data)) for x in split_index])\n",
        "\n",
        "    # Add split index (this makes splitting by group more efficient).\n",
        "    for i in range(len(ratios)):\n",
        "        splits[i][\"split_index\"] = i\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "def filter_k_core(data, core_num=0, col_user=\"userID\", col_item=\"itemID\"):\n",
        "    \"\"\"Filter rating dataframe for minimum number of users and items by\n",
        "    repeatedly applying min_rating_filter until the condition is satisfied.\n",
        "    \"\"\"\n",
        "    num_users, num_items = len(data[col_user].unique()), len(data[col_item].unique())\n",
        "    logger.info(\"Original: %d users and %d items\", num_users, num_items)\n",
        "    df_inp = data.copy()\n",
        "\n",
        "    if core_num > 0:\n",
        "        while True:\n",
        "            df_inp = min_rating_filter_pandas(\n",
        "                df_inp, min_rating=core_num, filter_by=\"item\"\n",
        "            )\n",
        "            df_inp = min_rating_filter_pandas(\n",
        "                df_inp, min_rating=core_num, filter_by=\"user\"\n",
        "            )\n",
        "            count_u = df_inp.groupby(col_user)[col_item].count()\n",
        "            count_i = df_inp.groupby(col_item)[col_user].count()\n",
        "            if (\n",
        "                len(count_i[count_i < core_num]) == 0\n",
        "                and len(count_u[count_u < core_num]) == 0\n",
        "            ):\n",
        "                break\n",
        "    df_inp = df_inp.sort_values(by=[col_user])\n",
        "    num_users = len(df_inp[col_user].unique())\n",
        "    num_items = len(df_inp[col_item].unique())\n",
        "    logger.info(\"Final: %d users and %d items\", num_users, num_items)\n",
        "\n",
        "    return df_inp"
      ],
      "metadata": {
        "id": "QYwowadYVtY6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _do_stratification(\n",
        "    data,\n",
        "    ratio=0.8,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    is_random=True,\n",
        "    seed=42,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "):\n",
        "    # A few preliminary checks.\n",
        "    if not (filter_by == \"user\" or filter_by == \"item\"):\n",
        "        raise ValueError(\"filter_by should be either 'user' or 'item'.\")\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    if col_user not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing User Col\")\n",
        "\n",
        "    if col_item not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing Item Col\")\n",
        "\n",
        "    if not is_random:\n",
        "        if col_timestamp not in data.columns:\n",
        "            raise ValueError(\"Schema of data not valid. Missing Timestamp Col\")\n",
        "\n",
        "    multi_split, ratio = process_split_ratio(ratio)\n",
        "\n",
        "    split_by_column = col_user if filter_by == \"user\" else col_item\n",
        "\n",
        "    ratio = ratio if multi_split else [ratio, 1 - ratio]\n",
        "\n",
        "    if min_rating > 1:\n",
        "        data = min_rating_filter_pandas(\n",
        "            data,\n",
        "            min_rating=min_rating,\n",
        "            filter_by=filter_by,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "        )\n",
        "\n",
        "    # Split by each group and aggregate splits together.\n",
        "    splits = []\n",
        "\n",
        "    # If it is for chronological splitting, the split will be performed in a random way.\n",
        "    df_grouped = (\n",
        "        data.sort_values(col_timestamp).groupby(split_by_column)\n",
        "        if is_random is False\n",
        "        else data.groupby(split_by_column)\n",
        "    )\n",
        "\n",
        "    for _, group in df_grouped:\n",
        "        group_splits = split_pandas_data_with_ratios(\n",
        "            group, ratio, shuffle=is_random, seed=seed\n",
        "        )\n",
        "\n",
        "        # Concatenate the list of split dataframes.\n",
        "        concat_group_splits = pd.concat(group_splits)\n",
        "\n",
        "        splits.append(concat_group_splits)\n",
        "\n",
        "    # Concatenate splits for all the groups together.\n",
        "    splits_all = pd.concat(splits)\n",
        "\n",
        "    # Take split by split_index\n",
        "    splits_list = [\n",
        "        splits_all[splits_all[\"split_index\"] == x].drop(\"split_index\", axis=1)\n",
        "        for x in range(len(ratio))\n",
        "    ]\n",
        "\n",
        "    return splits_list\n"
      ],
      "metadata": {
        "id": "byq7NHDJUsMV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def python_stratified_split(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Pandas stratified splitter.\n",
        "    For each user / item, the split function takes proportions of ratings which is\n",
        "    specified by the split ratio(s). The split is stratified.\n",
        "    Args:\n",
        "        data (pandas.DataFrame): Pandas DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two halves and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list is\n",
        "            provided and the ratios are not summed to 1, they will be normalized.\n",
        "        seed (int): Seed.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to\n",
        "            filter with min_rating.\n",
        "        col_user (str): column name of user IDs.\n",
        "        col_item (str): column name of item IDs.\n",
        "    Returns:\n",
        "        list: Splits of the input data as pandas.DataFrame.\n",
        "    \"\"\"\n",
        "    return _do_stratification(\n",
        "        data,\n",
        "        ratio=ratio,\n",
        "        min_rating=min_rating,\n",
        "        filter_by=filter_by,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        is_random=True,\n",
        "        seed=seed,\n",
        "    )"
      ],
      "metadata": {
        "id": "hez73jENTKLN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Definitions"
      ],
      "metadata": {
        "id": "_v_PXSVEWONp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import wraps\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    explained_variance_score,\n",
        "    roc_auc_score,\n",
        "    log_loss,\n",
        ")\n",
        "\n",
        "\n",
        "def _check_column_dtypes(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "    This includes the checks on:\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_wrapper(\n",
        "        rating_true,\n",
        "        rating_pred,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "        Args:\n",
        "            rating_true (pandas.DataFrame): True data\n",
        "            rating_pred (pandas.DataFrame): Predicted data\n",
        "            col_user (str): column name for user\n",
        "            col_item (str): column name for item\n",
        "            col_rating (str): column name for rating\n",
        "            col_prediction (str): column name for prediction\n",
        "        \"\"\"\n",
        "\n",
        "        if not has_columns(rating_true, [col_user, col_item, col_rating]):\n",
        "            raise ValueError(\"Missing columns in true rating DataFrame\")\n",
        "        if not has_columns(rating_pred, [col_user, col_item, col_prediction]):\n",
        "            raise ValueError(\"Missing columns in predicted rating DataFrame\")\n",
        "        if not has_same_base_dtype(\n",
        "            rating_true, rating_pred, columns=[col_user, col_item]\n",
        "        ):\n",
        "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
        "\n",
        "        return func(\n",
        "            rating_true=rating_true,\n",
        "            rating_pred=rating_pred,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            col_rating=col_rating,\n",
        "            col_prediction=col_prediction,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_wrapper\n",
        "\n",
        "\n",
        "@_check_column_dtypes\n",
        "@lru_cache_df(maxsize=1)\n",
        "def merge_rating_true_pred(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Join truth and prediction data frames on userID and itemID and return the true\n",
        "    and predicted rated with the correct index.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        numpy.ndarray: Array with the true ratings\n",
        "        numpy.ndarray: Array with the predicted ratings\n",
        "    \"\"\"\n",
        "\n",
        "    # pd.merge will apply suffixes to columns which have the same name across both dataframes\n",
        "    suffixes = [\"_true\", \"_pred\"]\n",
        "    rating_true_pred = pd.merge(\n",
        "        rating_true, rating_pred, on=[col_user, col_item], suffixes=suffixes\n",
        "    )\n",
        "    if col_rating in rating_pred.columns:\n",
        "        col_rating = col_rating + suffixes[0]\n",
        "    if col_prediction in rating_true.columns:\n",
        "        col_prediction = col_prediction + suffixes[1]\n",
        "    return rating_true_pred[col_rating], rating_true_pred[col_prediction]\n",
        "\n",
        "\n",
        "def rmse(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate Root Mean Squared Error\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: Root mean squared error\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def mae(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate Mean Absolute Error.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: Mean Absolute Error.\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "def rsquared(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate R squared\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: R squared (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return r2_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def exp_var(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate explained variance.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data. There should be no duplicate (userID, itemID) pairs\n",
        "        rating_pred (pandas.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: Explained variance (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return explained_variance_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def auc(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate the Area-Under-Curve metric for implicit feedback typed\n",
        "    recommender, where rating is binary and prediction is float number ranging\n",
        "    from 0 to 1.\n",
        "    https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
        "    Note:\n",
        "        The evaluation does not require a leave-one-out scenario.\n",
        "        This metric does not calculate group-based AUC which considers the AUC scores\n",
        "        averaged across users. It is also not limited to k. Instead, it calculates the\n",
        "        scores on the entire prediction results regardless the users.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: auc_score (min=0, max=1)\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return roc_auc_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def logloss(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "):\n",
        "    \"\"\"Calculate the logloss metric for implicit feedback typed\n",
        "    recommender, where rating is binary and prediction is float number ranging\n",
        "    from 0 to 1.\n",
        "    https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss)\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True data\n",
        "        rating_pred (pandas.DataFrame): Predicted data\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "    Returns:\n",
        "        float: log_loss_score (min=-inf, max=inf)\n",
        "    \"\"\"\n",
        "\n",
        "    y_true, y_pred = merge_rating_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "    )\n",
        "    return log_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "@_check_column_dtypes\n",
        "@lru_cache_df(maxsize=1)\n",
        "def merge_ranking_true_pred(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user,\n",
        "    col_item,\n",
        "    col_rating,\n",
        "    col_prediction,\n",
        "    relevancy_method,\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Filter truth and prediction data frames on common users\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user (optional)\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "    Returns:\n",
        "        pandas.DataFrame, pandas.DataFrame, int: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
        "        DataFrame of hit counts vs actual relevant items per user number of unique user ids\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the prediction and true data frames have the same set of users\n",
        "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
        "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
        "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
        "    n_users = len(common_users)\n",
        "\n",
        "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
        "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
        "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
        "    # to calculate penalized precision of the ordered items.\n",
        "    if relevancy_method == \"top_k\":\n",
        "        top_k = k\n",
        "    elif relevancy_method == \"by_threshold\":\n",
        "        top_k = threshold\n",
        "    elif relevancy_method is None:\n",
        "        top_k = None\n",
        "    else:\n",
        "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
        "    df_hit = get_top_k_items(\n",
        "        dataframe=rating_pred_common,\n",
        "        col_user=col_user,\n",
        "        col_rating=col_prediction,\n",
        "        k=top_k,\n",
        "    )\n",
        "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
        "        [col_user, col_item, \"rank\"]\n",
        "    ]\n",
        "\n",
        "    # count the number of hits vs actual relevant items per user\n",
        "    df_hit_count = pd.merge(\n",
        "        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
        "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
        "            {\"actual\": \"count\"}\n",
        "        ),\n",
        "        on=col_user,\n",
        "    )\n",
        "\n",
        "    return df_hit, df_hit_count, n_users\n",
        "\n",
        "\n",
        "def precision_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Precision at K.\n",
        "    Note:\n",
        "        We use the same formula to calculate precision@k as that in Spark.\n",
        "        More details can be found at\n",
        "        http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt\n",
        "        In particular, the maximum achievable precision may be < 1, if the number of items for a\n",
        "        user in rating_pred is less than k.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "    Returns:\n",
        "        float: precision at k (min=0, max=1)\n",
        "    \"\"\"\n",
        "\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (df_hit_count[\"hit\"] / k).sum() / n_users\n",
        "\n",
        "\n",
        "def recall_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Recall at K.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "    Returns:\n",
        "        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n",
        "        k items exist for a user in rating_true.\n",
        "    \"\"\"\n",
        "\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users\n",
        "\n",
        "\n",
        "def ndcg_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Normalized Discounted Cumulative Gain (nDCG).\n",
        "    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "    Returns:\n",
        "        float: nDCG at k (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # calculate discounted gain for hit items\n",
        "    df_dcg = df_hit.copy()\n",
        "    # relevance in this case is always 1\n",
        "    df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
        "    # sum up discount gained to get discount cumulative gain\n",
        "    df_dcg = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
        "    # calculate ideal discounted cumulative gain\n",
        "    df_ndcg = pd.merge(df_dcg, df_hit_count, on=[col_user])\n",
        "    df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
        "        lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n",
        "    )\n",
        "\n",
        "    # DCG over IDCG is the normalized DCG\n",
        "    return (df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users\n",
        "\n",
        "\n",
        "def map_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Mean Average Precision at k\n",
        "    The implementation of MAP is referenced from Spark MLlib evaluation metrics.\n",
        "    https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems\n",
        "    A good reference can be found at:\n",
        "    http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
        "    Note:\n",
        "        1. The evaluation function is named as 'MAP is at k' because the evaluation class takes top k items for\n",
        "        the prediction items. The naming is different from Spark.\n",
        "        2. The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of\n",
        "        relevant items in the ground truth data, instead of k.\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "    Returns:\n",
        "        float: MAP at k (min=0, max=1).\n",
        "    \"\"\"\n",
        "\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_rating=col_rating,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # calculate reciprocal rank of items for each user and sum them up\n",
        "    df_hit_sorted = df_hit.copy()\n",
        "    df_hit_sorted[\"rr\"] = (\n",
        "        df_hit_sorted.groupby(col_user).cumcount() + 1\n",
        "    ) / df_hit_sorted[\"rank\"]\n",
        "    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n",
        "\n",
        "    df_merge = pd.merge(df_hit_sorted, df_hit_count, on=col_user)\n",
        "    return (df_merge[\"rr\"] / df_merge[\"actual\"]).sum() / n_users\n",
        "\n",
        "\n",
        "def get_top_k_items(\n",
        "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
        "):\n",
        "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
        "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
        "    for each user.\n",
        "    Note:\n",
        "        If it is implicit rating, just append a column of constants to be\n",
        "        ratings.\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
        "        customerID-itemID-rating)\n",
        "        col_user (str): column name for user\n",
        "        col_rating (str): column name for rating\n",
        "        k (int or None): number of items for each user; None means that the input has already been\n",
        "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
        "    \"\"\"\n",
        "    # Sort dataframe by col_user and (top k) col_rating\n",
        "    if k is None:\n",
        "        top_k_items = dataframe\n",
        "    else:\n",
        "        top_k_items = (\n",
        "            dataframe.groupby(col_user, as_index=False)\n",
        "            .apply(lambda x: x.nlargest(k, col_rating))\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "    # Add ranks\n",
        "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
        "    return top_k_items\n",
        "\n",
        "\n",
        "\"\"\"Function name and function mapper.\n",
        "Useful when we have to serialize evaluation metric names\n",
        "and call the functions based on deserialized names\"\"\"\n",
        "metrics = {\n",
        "    rmse.__name__: rmse,\n",
        "    mae.__name__: mae,\n",
        "    rsquared.__name__: rsquared,\n",
        "    exp_var.__name__: exp_var,\n",
        "    precision_at_k.__name__: precision_at_k,\n",
        "    recall_at_k.__name__: recall_at_k,\n",
        "    ndcg_at_k.__name__: ndcg_at_k,\n",
        "    map_at_k.__name__: map_at_k,\n",
        "}\n",
        "\n",
        "\n",
        "# diversity metrics\n",
        "def _check_column_dtypes_diversity_serendipity(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "    This includes the checks on:\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "    * whether reco_df contains any user_item pairs that are already shown in train_df\n",
        "    * check relevance column in reco_df\n",
        "    * check column names in item_feature_df\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_diversity_serendipity_wrapper(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df=None,\n",
        "        item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "        col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_sim=DEFAULT_SIMILARITY_COL,\n",
        "        col_relevance=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "        Args:\n",
        "            train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "            reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "            item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "                It contains two columns: col_item and features (a feature vector).\n",
        "            item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "                Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "            col_item_features (str): item feature column name.\n",
        "            col_user (str): User id column name.\n",
        "            col_item (str): Item id column name.\n",
        "            col_sim (str): This column indicates the column name for item similarity.\n",
        "            col_relevance (str): This column indicates whether the recommended item is actually\n",
        "                relevant to the user or not.\n",
        "        \"\"\"\n",
        "\n",
        "        if not has_columns(train_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
        "        if not has_columns(reco_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
        "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
        "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
        "        if col_relevance is None:\n",
        "            col_relevance = DEFAULT_RELEVANCE_COL\n",
        "            # relevance term, default is 1 (relevant) for all\n",
        "            reco_df = reco_df[[col_user, col_item]]\n",
        "            reco_df[col_relevance] = 1.0\n",
        "        else:\n",
        "            col_relevance = col_relevance\n",
        "            reco_df = reco_df[[col_user, col_item, col_relevance]].astype(\n",
        "                {col_relevance: np.float16}\n",
        "            )\n",
        "        if item_sim_measure == \"item_feature_vector\":\n",
        "            required_columns = [col_item, col_item_features]\n",
        "            if item_feature_df is not None:\n",
        "                if not has_columns(item_feature_df, required_columns):\n",
        "                    raise ValueError(\"Missing columns in item_feature_df DataFrame\")\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"item_feature_df not specified! item_feature_df must be provided \"\n",
        "                    \"if choosing to use item_feature_vector to calculate item similarity. \"\n",
        "                    \"item_feature_df should have columns: \" + str(required_columns)\n",
        "                )\n",
        "        # check if reco_df contains any user_item pairs that are already shown in train_df\n",
        "        count_intersection = pd.merge(\n",
        "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
        "        ).shape[0]\n",
        "        if count_intersection != 0:\n",
        "            raise Exception(\n",
        "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            train_df=train_df,\n",
        "            reco_df=reco_df,\n",
        "            item_feature_df=item_feature_df,\n",
        "            item_sim_measure=item_sim_measure,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            col_sim=col_sim,\n",
        "            col_relevance=col_relevance,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_diversity_serendipity_wrapper\n",
        "\n",
        "\n",
        "def _check_column_dtypes_novelty_coverage(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "    This includes the checks on:\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "    * whether reco_df contains any user_item pairs that are already shown in train_df\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_novelty_coverage_wrapper(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "        Args:\n",
        "            train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "            reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "            col_user (str): User id column name.\n",
        "            col_item (str): Item id column name.\n",
        "        \"\"\"\n",
        "\n",
        "        if not has_columns(train_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
        "        if not has_columns(reco_df, [col_user, col_item]):\n",
        "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
        "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
        "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
        "\n",
        "        count_intersection = pd.merge(\n",
        "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
        "        ).shape[0]\n",
        "        if count_intersection != 0:\n",
        "            raise Exception(\n",
        "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            train_df=train_df,\n",
        "            reco_df=reco_df,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_novelty_coverage_wrapper\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_pairwise_items(\n",
        "    df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Get pairwise combinations of items per user (ignoring duplicate pairs [1,2] == [2,1])\"\"\"\n",
        "    df_user_i1 = df[[col_user, col_item]]\n",
        "    df_user_i1.columns = [col_user, \"i1\"]\n",
        "\n",
        "    df_user_i2 = df[[col_user, col_item]]\n",
        "    df_user_i2.columns = [col_user, \"i2\"]\n",
        "\n",
        "    df_user_i1_i2 = pd.merge(df_user_i1, df_user_i2, how=\"inner\", on=[col_user])\n",
        "\n",
        "    df_pairwise_items = df_user_i1_i2[(df_user_i1_i2[\"i1\"] <= df_user_i1_i2[\"i2\"])][\n",
        "        [col_user, \"i1\", \"i2\"]\n",
        "    ].reset_index(drop=True)\n",
        "    return df_pairwise_items\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_cosine_similarity(\n",
        "    train_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "\n",
        "    if item_sim_measure == \"item_cooccurrence_count\":\n",
        "        # calculate item-item similarity based on item co-occurrence count\n",
        "        df_cosine_similarity = _get_cooccurrence_similarity(\n",
        "            train_df, col_user, col_item, col_sim\n",
        "        )\n",
        "    elif item_sim_measure == \"item_feature_vector\":\n",
        "        # calculdf_cosine_similarity = ate item-item similarity based on item feature vectors\n",
        "        df_cosine_similarity = _get_item_feature_similarity(\n",
        "            item_feature_df, col_item_features, col_user, col_item\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(\n",
        "            \"item_sim_measure not recognized! The available options include 'item_cooccurrence_count' and 'item_feature_vector'.\"\n",
        "        )\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_cooccurrence_similarity(\n",
        "    train_df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Cosine similarity metric from\n",
        "    :Citation:\n",
        "        Y.C. Zhang, D.Ã“. SÃ©aghdha, D. Quercia and T. Jambor, Auralist:\n",
        "        introducing serendipity into music recommendation, WSDM 2012\n",
        "    The item indexes in the result are such that i1 <= i2.\n",
        "    \"\"\"\n",
        "    pairs = _get_pairwise_items(train_df, col_user, col_item)\n",
        "    pairs_count = pd.DataFrame(\n",
        "        {\"count\": pairs.groupby([\"i1\", \"i2\"]).size()}\n",
        "    ).reset_index()\n",
        "    item_count = pd.DataFrame(\n",
        "        {\"count\": train_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    item_count[\"item_sqrt_count\"] = item_count[\"count\"] ** 0.5\n",
        "    item_co_occur = pairs_count.merge(\n",
        "        item_count[[col_item, \"item_sqrt_count\"]],\n",
        "        left_on=[\"i1\"],\n",
        "        right_on=[col_item],\n",
        "    ).drop(columns=[col_item])\n",
        "\n",
        "    item_co_occur.columns = [\"i1\", \"i2\", \"count\", \"i1_sqrt_count\"]\n",
        "\n",
        "    item_co_occur = item_co_occur.merge(\n",
        "        item_count[[col_item, \"item_sqrt_count\"]],\n",
        "        left_on=[\"i2\"],\n",
        "        right_on=[col_item],\n",
        "    ).drop(columns=[col_item])\n",
        "    item_co_occur.columns = [\n",
        "        \"i1\",\n",
        "        \"i2\",\n",
        "        \"count\",\n",
        "        \"i1_sqrt_count\",\n",
        "        \"i2_sqrt_count\",\n",
        "    ]\n",
        "\n",
        "    item_co_occur[col_sim] = item_co_occur[\"count\"] / (\n",
        "        item_co_occur[\"i1_sqrt_count\"] * item_co_occur[\"i2_sqrt_count\"]\n",
        "    )\n",
        "    df_cosine_similarity = (\n",
        "        item_co_occur[[\"i1\", \"i2\", col_sim]]\n",
        "        .sort_values([\"i1\", \"i2\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_item_feature_similarity(\n",
        "    item_feature_df,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Cosine similarity metric based on item feature vectors\n",
        "    The item indexes in the result are such that i1 <= i2.\n",
        "    \"\"\"\n",
        "    df1 = item_feature_df[[col_item, col_item_features]]\n",
        "    df1.columns = [\"i1\", \"f1\"]\n",
        "    df1[\"key\"] = 0\n",
        "    df2 = item_feature_df[[col_item, col_item_features]]\n",
        "    df2.columns = [\"i2\", \"f2\"]\n",
        "    df2[\"key\"] = 0\n",
        "\n",
        "    df = pd.merge(df1, df2, on=\"key\", how=\"outer\").drop(\"key\", axis=1)\n",
        "    df_item_feature_pair = df[(df[\"i1\"] <= df[\"i2\"])].reset_index(drop=True)\n",
        "\n",
        "    df_item_feature_pair[col_sim] = df_item_feature_pair.apply(\n",
        "        lambda x: float(x.f1.dot(x.f2))\n",
        "        / float(np.linalg.norm(x.f1, 2) * np.linalg.norm(x.f2, 2)),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    df_cosine_similarity = df_item_feature_pair[[\"i1\", \"i2\", col_sim]].sort_values(\n",
        "        [\"i1\", \"i2\"]\n",
        "    )\n",
        "\n",
        "    return df_cosine_similarity\n",
        "\n",
        "\n",
        "# Diversity metrics\n",
        "@lru_cache_df(maxsize=1)\n",
        "def _get_intralist_similarity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "):\n",
        "    \"\"\"Intra-list similarity from\n",
        "    :Citation:\n",
        "        \"Improving Recommendation Lists Through Topic Diversification\",\n",
        "        Ziegler, McNee, Konstan and Lausen, 2005.\n",
        "    \"\"\"\n",
        "    pairs = _get_pairwise_items(reco_df, col_user, col_item)\n",
        "    similarity_df = _get_cosine_similarity(\n",
        "        train_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    # Fillna(0) is needed in the cases where similarity_df does not have an entry for a pair of items.\n",
        "    # e.g. i1 and i2 have never occurred together.\n",
        "\n",
        "    item_pair_sim = pairs.merge(similarity_df, on=[\"i1\", \"i2\"], how=\"left\")\n",
        "    item_pair_sim[col_sim].fillna(0, inplace=True)\n",
        "    item_pair_sim = item_pair_sim.loc[\n",
        "        item_pair_sim[\"i1\"] != item_pair_sim[\"i2\"]\n",
        "    ].reset_index(drop=True)\n",
        "    df_intralist_similarity = (\n",
        "        item_pair_sim.groupby([col_user]).agg({col_sim: \"mean\"}).reset_index()\n",
        "    )\n",
        "    df_intralist_similarity.columns = [col_user, \"avg_il_sim\"]\n",
        "\n",
        "    return df_intralist_similarity\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "@lru_cache_df(maxsize=1)\n",
        "def user_diversity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average diversity of recommendations for each user.\n",
        "    The metric definition is based on formula (3) in the following reference:\n",
        "    :Citation:\n",
        "        Y.C. Zhang, D.Ã“. SÃ©aghdha, D. Quercia and T. Jambor, Auralist:\n",
        "        introducing serendipity into music recommendation, WSDM 2012\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they have interacted with;\n",
        "            contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item, col_relevance (optional).\n",
        "            Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually relevant to the user or not.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with the following columns: col_user, user_diversity.\n",
        "    \"\"\"\n",
        "\n",
        "    df_intralist_similarity = _get_intralist_similarity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    df_user_diversity = df_intralist_similarity\n",
        "    df_user_diversity[\"user_diversity\"] = 1 - df_user_diversity[\"avg_il_sim\"]\n",
        "    df_user_diversity = (\n",
        "        df_user_diversity[[col_user, \"user_diversity\"]]\n",
        "        .sort_values(col_user)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_user_diversity\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def diversity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average diversity of recommendations across all users.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they have interacted with;\n",
        "            contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item, col_relevance (optional).\n",
        "            Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually relevant to the user or not.\n",
        "    Returns:\n",
        "        float: diversity.\n",
        "    \"\"\"\n",
        "    df_user_diversity = user_diversity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    avg_diversity = df_user_diversity.agg({\"user_diversity\": \"mean\"})[0]\n",
        "    return avg_diversity\n",
        "\n",
        "\n",
        "# Novelty metrics\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "@lru_cache_df(maxsize=1)\n",
        "def historical_item_novelty(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Calculate novelty for each item. Novelty is computed as the minus logarithm of\n",
        "    (number of interactions with item / total number of interactions). The definition of the metric\n",
        "    is based on the following reference using the choice model (eqs. 1 and 6):\n",
        "    :Citation:\n",
        "        P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:\n",
        "        choice, discovery and relevance, ECIR 2011\n",
        "    The novelty of an item can be defined relative to a set of observed events on the set of all items.\n",
        "    These can be events of user choice (item \"is picked\" by a random user) or user discovery\n",
        "    (item \"is known\" to a random user). The above definition of novelty reflects a factor of item popularity.\n",
        "    High novelty values correspond to long-tail items in the density function, that few users have interacted\n",
        "    with and low novelty values correspond to popular head items.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with the following columns: col_item, item_novelty.\n",
        "    \"\"\"\n",
        "\n",
        "    n_records = train_df.shape[0]\n",
        "    item_count = pd.DataFrame(\n",
        "        {\"count\": train_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    item_count[\"item_novelty\"] = -np.log2(item_count[\"count\"] / n_records)\n",
        "    df_item_novelty = (\n",
        "        item_count[[col_item, \"item_novelty\"]]\n",
        "        .sort_values(col_item)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_item_novelty\n",
        "\n",
        "\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def novelty(train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL):\n",
        "    \"\"\"Calculate the average novelty in a list of recommended items (this assumes that the recommendation list\n",
        "    is already computed). Follows section 5 from\n",
        "    :Citation:\n",
        "        P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:\n",
        "        choice, discovery and relevance, ECIR 2011\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "    Returns:\n",
        "        float: novelty.\n",
        "    \"\"\"\n",
        "\n",
        "    df_item_novelty = historical_item_novelty(train_df, reco_df, col_user, col_item)\n",
        "    n_recommendations = reco_df.shape[0]\n",
        "    reco_item_count = pd.DataFrame(\n",
        "        {\"count\": reco_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "    reco_item_novelty = reco_item_count.merge(df_item_novelty, on=col_item)\n",
        "    reco_item_novelty[\"product\"] = (\n",
        "        reco_item_novelty[\"count\"] * reco_item_novelty[\"item_novelty\"]\n",
        "    )\n",
        "    avg_novelty = reco_item_novelty.agg({\"product\": \"sum\"})[0] / n_recommendations\n",
        "\n",
        "    return avg_novelty\n",
        "\n",
        "\n",
        "# Serendipity metrics\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "@lru_cache_df(maxsize=1)\n",
        "def user_item_serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate serendipity of each item in the recommendations for each user.\n",
        "    The metric definition is based on the following references:\n",
        "    :Citation:\n",
        "    Y.C. Zhang, D.Ã“. SÃ©aghdha, D. Quercia and T. Jambor, Auralist:\n",
        "    introducing serendipity into music recommendation, WSDM 2012\n",
        "    Eugene Yan, Serendipity: Accuracyâ€™s unpopular best friend in Recommender Systems,\n",
        "    eugeneyan.com, April 2020\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with columns: col_user, col_item, user_item_serendipity.\n",
        "    \"\"\"\n",
        "    # for every col_user, col_item in reco_df, join all interacted items from train_df.\n",
        "    # These interacted items are repeated for each item in reco_df for a specific user.\n",
        "    df_cosine_similarity = _get_cosine_similarity(\n",
        "        train_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "    )\n",
        "    reco_user_item = reco_df[[col_user, col_item]]\n",
        "    reco_user_item[\"reco_item_tmp\"] = reco_user_item[col_item]\n",
        "\n",
        "    train_user_item = train_df[[col_user, col_item]]\n",
        "    train_user_item.columns = [col_user, \"train_item_tmp\"]\n",
        "\n",
        "    reco_train_user_item = reco_user_item.merge(train_user_item, on=[col_user])\n",
        "    reco_train_user_item[\"i1\"] = reco_train_user_item[\n",
        "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
        "    ].min(axis=1)\n",
        "    reco_train_user_item[\"i2\"] = reco_train_user_item[\n",
        "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
        "    ].max(axis=1)\n",
        "\n",
        "    reco_train_user_item_sim = reco_train_user_item.merge(\n",
        "        df_cosine_similarity, on=[\"i1\", \"i2\"], how=\"left\"\n",
        "    )\n",
        "    reco_train_user_item_sim[col_sim].fillna(0, inplace=True)\n",
        "\n",
        "    reco_user_item_avg_sim = (\n",
        "        reco_train_user_item_sim.groupby([col_user, col_item])\n",
        "        .agg({col_sim: \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "    reco_user_item_avg_sim.columns = [\n",
        "        col_user,\n",
        "        col_item,\n",
        "        \"avg_item2interactedHistory_sim\",\n",
        "    ]\n",
        "\n",
        "    df_user_item_serendipity = reco_user_item_avg_sim.merge(\n",
        "        reco_df, on=[col_user, col_item]\n",
        "    )\n",
        "    df_user_item_serendipity[\"user_item_serendipity\"] = (\n",
        "        1 - df_user_item_serendipity[\"avg_item2interactedHistory_sim\"]\n",
        "    ) * df_user_item_serendipity[col_relevance]\n",
        "    df_user_item_serendipity = (\n",
        "        df_user_item_serendipity[[col_user, col_item, \"user_item_serendipity\"]]\n",
        "        .sort_values([col_user, col_item])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return df_user_item_serendipity\n",
        "\n",
        "\n",
        "@lru_cache_df(maxsize=1)\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def user_serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average serendipity for each user's recommendations.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A dataframe with following columns: col_user, user_serendipity.\n",
        "    \"\"\"\n",
        "    df_user_item_serendipity = user_item_serendipity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "        col_relevance,\n",
        "    )\n",
        "    df_user_serendipity = (\n",
        "        df_user_item_serendipity.groupby(col_user)\n",
        "        .agg({\"user_item_serendipity\": \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "    df_user_serendipity.columns = [col_user, \"user_serendipity\"]\n",
        "    df_user_serendipity = df_user_serendipity.sort_values(col_user).reset_index(\n",
        "        drop=True\n",
        "    )\n",
        "\n",
        "    return df_user_serendipity\n",
        "\n",
        "\n",
        "@_check_column_dtypes_diversity_serendipity\n",
        "def serendipity(\n",
        "    train_df,\n",
        "    reco_df,\n",
        "    item_feature_df=None,\n",
        "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_sim=DEFAULT_SIMILARITY_COL,\n",
        "    col_relevance=None,\n",
        "):\n",
        "    \"\"\"Calculate average serendipity for recommendations across all users.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "              have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "              col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        item_feature_df (pandas.DataFrame): (Optional) It is required only when item_sim_measure='item_feature_vector'.\n",
        "            It contains two columns: col_item and features (a feature vector).\n",
        "        item_sim_measure (str): (Optional) This column indicates which item similarity measure to be used.\n",
        "            Available measures include item_cooccurrence_count (default choice) and item_feature_vector.\n",
        "        col_item_features (str): item feature column name.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "        col_sim (str): This column indicates the column name for item similarity.\n",
        "        col_relevance (str): This column indicates whether the recommended item is actually\n",
        "              relevant to the user or not.\n",
        "    Returns:\n",
        "        float: serendipity.\n",
        "    \"\"\"\n",
        "    df_user_serendipity = user_serendipity(\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df,\n",
        "        item_sim_measure,\n",
        "        col_item_features,\n",
        "        col_user,\n",
        "        col_item,\n",
        "        col_sim,\n",
        "        col_relevance,\n",
        "    )\n",
        "    avg_serendipity = df_user_serendipity.agg({\"user_serendipity\": \"mean\"})[0]\n",
        "    return avg_serendipity\n",
        "\n",
        "\n",
        "# Coverage metrics\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def catalog_coverage(\n",
        "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
        "):\n",
        "    \"\"\"Calculate catalog coverage for recommendations across all users.\n",
        "    The metric definition is based on the \"catalog coverage\" definition in the following reference:\n",
        "    :Citation:\n",
        "        G. Shani and A. Gunawardana, Evaluating Recommendation Systems,\n",
        "        Recommender Systems Handbook pp. 257-297, 2010.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "    Returns:\n",
        "        float: catalog coverage\n",
        "    \"\"\"\n",
        "    # distinct item count in reco_df\n",
        "    count_distinct_item_reco = reco_df[col_item].nunique()\n",
        "    # distinct item count in train_df\n",
        "    count_distinct_item_train = train_df[col_item].nunique()\n",
        "\n",
        "    # catalog coverage\n",
        "    c_coverage = count_distinct_item_reco / count_distinct_item_train\n",
        "    return c_coverage\n",
        "\n",
        "\n",
        "@_check_column_dtypes_novelty_coverage\n",
        "def distributional_coverage(\n",
        "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
        "):\n",
        "    \"\"\"Calculate distributional coverage for recommendations across all users.\n",
        "    The metric definition is based on formula (21) in the following reference:\n",
        "    :Citation:\n",
        "        G. Shani and A. Gunawardana, Evaluating Recommendation Systems,\n",
        "        Recommender Systems Handbook pp. 257-297, 2010.\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Data set with historical data for users and items they\n",
        "                have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.\n",
        "                Interaction here follows the *item choice model* from Castells et al.\n",
        "        reco_df (pandas.DataFrame): Recommender's prediction output, containing col_user, col_item,\n",
        "                col_relevance (optional). Assumed to not contain any duplicate user-item pairs.\n",
        "        col_user (str): User id column name.\n",
        "        col_item (str): Item id column name.\n",
        "    Returns:\n",
        "        float: distributional coverage\n",
        "    \"\"\"\n",
        "    # In reco_df, how  many times each col_item is being recommended\n",
        "    df_itemcnt_reco = pd.DataFrame(\n",
        "        {\"count\": reco_df.groupby([col_item]).size()}\n",
        "    ).reset_index()\n",
        "\n",
        "    # the number of total recommendations\n",
        "    count_row_reco = reco_df.shape[0]\n",
        "\n",
        "    df_entropy = df_itemcnt_reco\n",
        "    df_entropy[\"p(i)\"] = df_entropy[\"count\"] / count_row_reco\n",
        "    df_entropy[\"entropy(i)\"] = df_entropy[\"p(i)\"] * np.log2(df_entropy[\"p(i)\"])\n",
        "\n",
        "    d_coverage = -df_entropy.agg({\"entropy(i)\": \"sum\"})[0]\n",
        "\n",
        "    return d_coverage"
      ],
      "metadata": {
        "id": "U4iY48VtcBcI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAR Model"
      ],
      "metadata": {
        "id": "gmQKDm_NduJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from scipy import sparse\n",
        "\n",
        "COOCCUR = \"cooccurrence\"\n",
        "JACCARD = \"jaccard\"\n",
        "LIFT = \"lift\"\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "class SARSingleNode:\n",
        "    \"\"\"Simple Algorithm for Recommendations (SAR) implementation\n",
        "    SAR is a fast scalable adaptive algorithm for personalized recommendations based on user transaction history\n",
        "    and items description. The core idea behind SAR is to recommend items like those that a user already has\n",
        "    demonstrated an affinity to. It does this by 1) estimating the affinity of users for items, 2) estimating\n",
        "    similarity across items, and then 3) combining the estimates to generate a set of recommendations for a given user.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        similarity_type=JACCARD,\n",
        "        time_decay_coefficient=30,\n",
        "        time_now=None,\n",
        "        timedecay_formula=False,\n",
        "        threshold=1,\n",
        "        normalize=False,\n",
        "    ):\n",
        "        \"\"\"Initialize model parameters\n",
        "        Args:\n",
        "            col_user (str): user column name\n",
        "            col_item (str): item column name\n",
        "            col_rating (str): rating column name\n",
        "            col_timestamp (str): timestamp column name\n",
        "            col_prediction (str): prediction column name\n",
        "            similarity_type (str): ['cooccurrence', 'jaccard', 'lift'] option for computing item-item similarity\n",
        "            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\n",
        "            time_now (int | None): current time for time decay calculation\n",
        "            timedecay_formula (bool): flag to apply time decay\n",
        "            threshold (int): item-item co-occurrences below this threshold will be removed\n",
        "            normalize (bool): option for normalizing predictions to scale of original ratings\n",
        "        \"\"\"\n",
        "        self.col_rating = col_rating\n",
        "        self.col_item = col_item\n",
        "        self.col_user = col_user\n",
        "        self.col_timestamp = col_timestamp\n",
        "        self.col_prediction = col_prediction\n",
        "\n",
        "        if similarity_type not in [COOCCUR, JACCARD, LIFT]:\n",
        "            raise ValueError(\n",
        "                'Similarity type must be one of [\"cooccurrence\" | \"jaccard\" | \"lift\"]'\n",
        "            )\n",
        "        self.similarity_type = similarity_type\n",
        "        self.time_decay_half_life = (\n",
        "            time_decay_coefficient * 24 * 60 * 60\n",
        "        )  # convert to seconds\n",
        "        self.time_decay_flag = timedecay_formula\n",
        "        self.time_now = time_now\n",
        "        self.threshold = threshold\n",
        "        self.user_affinity = None\n",
        "        self.item_similarity = None\n",
        "        self.item_frequencies = None\n",
        "\n",
        "        # threshold - items below this number get set to zero in co-occurrence counts\n",
        "        if self.threshold <= 0:\n",
        "            raise ValueError(\"Threshold cannot be < 1\")\n",
        "\n",
        "        # set flag to capture unity-rating user-affinity matrix for scaling scores\n",
        "        self.normalize = normalize\n",
        "        self.col_unity_rating = \"_unity_rating\"\n",
        "        self.unity_user_affinity = None\n",
        "\n",
        "        # column for mapping user / item ids to internal indices\n",
        "        self.col_item_id = \"_indexed_items\"\n",
        "        self.col_user_id = \"_indexed_users\"\n",
        "\n",
        "        # obtain all the users and items from both training and test data\n",
        "        self.n_users = None\n",
        "        self.n_items = None\n",
        "\n",
        "        # The min and max of the rating scale, obtained from the training data.\n",
        "        self.rating_min = None\n",
        "        self.rating_max = None\n",
        "\n",
        "        # mapping for item to matrix element\n",
        "        self.user2index = None\n",
        "        self.item2index = None\n",
        "\n",
        "        # the opposite of the above map - map array index to actual string ID\n",
        "        self.index2item = None\n",
        "\n",
        "    def compute_affinity_matrix(self, df, rating_col):\n",
        "        \"\"\"Affinity matrix.\n",
        "        The user-affinity matrix can be constructed by treating the users and items as\n",
        "        indices in a sparse matrix, and the events as the data. Here, we're treating\n",
        "        the ratings as the event weights.  We convert between different sparse-matrix\n",
        "        formats to de-duplicate user-item pairs, otherwise they will get added up.\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Indexed df of users and items\n",
        "            rating_col (str): Name of column to use for ratings\n",
        "        Returns:\n",
        "            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\n",
        "        \"\"\"\n",
        "\n",
        "        return sparse.coo_matrix(\n",
        "            (df[rating_col], (df[self.col_user_id], df[self.col_item_id])),\n",
        "            shape=(self.n_users, self.n_items),\n",
        "        ).tocsr()\n",
        "\n",
        "    def compute_time_decay(self, df, decay_column):\n",
        "        \"\"\"Compute time decay on provided column.\n",
        "        Args:\n",
        "            df (pandas.DataFrame): DataFrame of users and items\n",
        "            decay_column (str): column to decay\n",
        "        Returns:\n",
        "            pandas.DataFrame: with column decayed\n",
        "        \"\"\"\n",
        "\n",
        "        # if time_now is None use the latest time\n",
        "        if self.time_now is None:\n",
        "            self.time_now = df[self.col_timestamp].max()\n",
        "\n",
        "        # apply time decay to each rating\n",
        "        df[decay_column] *= exponential_decay(\n",
        "            value=df[self.col_timestamp],\n",
        "            max_val=self.time_now,\n",
        "            half_life=self.time_decay_half_life,\n",
        "        )\n",
        "\n",
        "        # group time decayed ratings by user-item and take the sum as the user-item affinity\n",
        "        return df.groupby([self.col_user, self.col_item]).sum().reset_index()\n",
        "\n",
        "    def compute_cooccurrence_matrix(self, df):\n",
        "        \"\"\"Co-occurrence matrix.\n",
        "        The co-occurrence matrix is defined as :math:`C = U^T * U`\n",
        "        where U is the user_affinity matrix with 1's as values (instead of ratings).\n",
        "        Args:\n",
        "            df (pandas.DataFrame): DataFrame of users and items\n",
        "        Returns:\n",
        "            numpy.ndarray: Co-occurrence matrix\n",
        "        \"\"\"\n",
        "\n",
        "        user_item_hits = sparse.coo_matrix(\n",
        "            (np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])),\n",
        "            shape=(self.n_users, self.n_items),\n",
        "        ).tocsr()\n",
        "\n",
        "        item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n",
        "        item_cooccurrence = item_cooccurrence.multiply(\n",
        "            item_cooccurrence >= self.threshold\n",
        "        )\n",
        "\n",
        "        return item_cooccurrence.astype(df[self.col_rating].dtype)\n",
        "\n",
        "    def set_index(self, df):\n",
        "        \"\"\"Generate continuous indices for users and items to reduce memory usage.\n",
        "        Args:\n",
        "            df (pandas.DataFrame): dataframe with user and item ids\n",
        "        \"\"\"\n",
        "\n",
        "        # generate a map of continuous index values to items\n",
        "        self.index2item = dict(enumerate(df[self.col_item].unique()))\n",
        "\n",
        "        # invert the mapping from above\n",
        "        self.item2index = {v: k for k, v in self.index2item.items()}\n",
        "\n",
        "        # create mapping of users to continuous indices\n",
        "        self.user2index = {x[1]: x[0] for x in enumerate(df[self.col_user].unique())}\n",
        "\n",
        "        # set values for the total count of users and items\n",
        "        self.n_users = len(self.user2index)\n",
        "        self.n_items = len(self.index2item)\n",
        "\n",
        "    def fit(self, df):\n",
        "        \"\"\"Main fit method for SAR.\n",
        "        .. note::\n",
        "        Please make sure that `df` has no duplicates.\n",
        "        Args:\n",
        "            df (pandas.DataFrame): User item rating dataframe (without duplicates).\n",
        "        \"\"\"\n",
        "\n",
        "        # generate continuous indices if this hasn't been done\n",
        "        if self.index2item is None:\n",
        "            self.set_index(df)\n",
        "\n",
        "        logger.info(\"Collecting user affinity matrix\")\n",
        "        if not np.issubdtype(df[self.col_rating].dtype, np.number):\n",
        "            raise TypeError(\"Rating column data type must be numeric\")\n",
        "\n",
        "        # copy the DataFrame to avoid modification of the input\n",
        "        select_columns = [self.col_user, self.col_item, self.col_rating]\n",
        "        if self.time_decay_flag:\n",
        "            select_columns += [self.col_timestamp]\n",
        "        temp_df = df[select_columns].copy()\n",
        "\n",
        "        if self.time_decay_flag:\n",
        "            logger.info(\"Calculating time-decayed affinities\")\n",
        "            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n",
        "\n",
        "        logger.info(\"Creating index columns\")\n",
        "        # add mapping of user and item ids to indices\n",
        "        temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(\n",
        "            lambda item: self.item2index.get(item, np.NaN)\n",
        "        )\n",
        "        temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(\n",
        "            lambda user: self.user2index.get(user, np.NaN)\n",
        "        )\n",
        "\n",
        "        if self.normalize:\n",
        "            self.rating_min = temp_df[self.col_rating].min()\n",
        "            self.rating_max = temp_df[self.col_rating].max()\n",
        "            logger.info(\"Calculating normalization factors\")\n",
        "            temp_df[self.col_unity_rating] = 1.0\n",
        "            if self.time_decay_flag:\n",
        "                temp_df = self.compute_time_decay(\n",
        "                    df=temp_df, decay_column=self.col_unity_rating\n",
        "                )\n",
        "            self.unity_user_affinity = self.compute_affinity_matrix(\n",
        "                df=temp_df, rating_col=self.col_unity_rating\n",
        "            )\n",
        "\n",
        "        # affinity matrix\n",
        "        logger.info(\"Building user affinity sparse matrix\")\n",
        "        self.user_affinity = self.compute_affinity_matrix(\n",
        "            df=temp_df, rating_col=self.col_rating\n",
        "        )\n",
        "\n",
        "        # calculate item co-occurrence\n",
        "        logger.info(\"Calculating item co-occurrence\")\n",
        "        item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n",
        "\n",
        "        # free up some space\n",
        "        del temp_df\n",
        "\n",
        "        self.item_frequencies = item_cooccurrence.diagonal()\n",
        "\n",
        "        logger.info(\"Calculating item similarity\")\n",
        "        if self.similarity_type == COOCCUR:\n",
        "            logger.info(\"Using co-occurrence based similarity\")\n",
        "            self.item_similarity = item_cooccurrence\n",
        "        elif self.similarity_type == JACCARD:\n",
        "            logger.info(\"Using jaccard based similarity\")\n",
        "            self.item_similarity = jaccard(item_cooccurrence).astype(\n",
        "                df[self.col_rating].dtype\n",
        "            )\n",
        "        elif self.similarity_type == LIFT:\n",
        "            logger.info(\"Using lift based similarity\")\n",
        "            self.item_similarity = lift(item_cooccurrence).astype(\n",
        "                df[self.col_rating].dtype\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown similarity type: {}\".format(self.similarity_type))\n",
        "\n",
        "        # free up some space\n",
        "        del item_cooccurrence\n",
        "\n",
        "        logger.info(\"Done training\")\n",
        "\n",
        "    def score(self, test, remove_seen=False):\n",
        "        \"\"\"Score all items for test users.\n",
        "        Args:\n",
        "            test (pandas.DataFrame): user to test\n",
        "            remove_seen (bool): flag to remove items seen in training from recommendation\n",
        "        Returns:\n",
        "            numpy.ndarray: Value of interest of all items for the users.\n",
        "        \"\"\"\n",
        "\n",
        "        # get user / item indices from test set\n",
        "        user_ids = list(\n",
        "            map(\n",
        "                lambda user: self.user2index.get(user, np.NaN),\n",
        "                test[self.col_user].unique(),\n",
        "            )\n",
        "        )\n",
        "        if any(np.isnan(user_ids)):\n",
        "            raise ValueError(\"SAR cannot score users that are not in the training set\")\n",
        "\n",
        "        # calculate raw scores with a matrix multiplication\n",
        "        logger.info(\"Calculating recommendation scores\")\n",
        "        test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n",
        "\n",
        "        # ensure we're working with a dense ndarray\n",
        "        if isinstance(test_scores, sparse.spmatrix):\n",
        "            test_scores = test_scores.toarray()\n",
        "\n",
        "        if self.normalize:\n",
        "            counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n",
        "            user_min_scores = (\n",
        "                np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1])\n",
        "                * self.rating_min\n",
        "            )\n",
        "            user_max_scores = (\n",
        "                np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1])\n",
        "                * self.rating_max\n",
        "            )\n",
        "            test_scores = rescale(\n",
        "                test_scores,\n",
        "                self.rating_min,\n",
        "                self.rating_max,\n",
        "                user_min_scores,\n",
        "                user_max_scores,\n",
        "            )\n",
        "\n",
        "        # remove items in the train set so recommended items are always novel\n",
        "        if remove_seen:\n",
        "            logger.info(\"Removing seen items\")\n",
        "            test_scores += self.user_affinity[user_ids, :] * -np.inf\n",
        "\n",
        "        return test_scores\n",
        "\n",
        "    def get_popularity_based_topk(self, top_k=10, sort_top_k=True):\n",
        "        \"\"\"Get top K most frequently occurring items across all users.\n",
        "        Args:\n",
        "            top_k (int): number of top items to recommend.\n",
        "            sort_top_k (bool): flag to sort top k results.\n",
        "        Returns:\n",
        "            pandas.DataFrame: top k most popular items.\n",
        "        \"\"\"\n",
        "\n",
        "        test_scores = np.array([self.item_frequencies])\n",
        "\n",
        "        logger.info(\"Getting top K\")\n",
        "        top_items, top_scores = get_top_k_scored_items(\n",
        "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
        "        )\n",
        "\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
        "                self.col_prediction: top_scores.flatten(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n",
        "        \"\"\"Get top K similar items to provided seed items based on similarity metric defined.\n",
        "        This method will take a set of items and use them to recommend the most similar items to that set\n",
        "        based on the similarity matrix fit during training.\n",
        "        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\n",
        "        The following options are possible based on information provided in the items input:\n",
        "        1. Single user or seed of items: only item column (ratings are assumed to be 1)\n",
        "        2. Single user or seed of items w/ ratings: item column and rating column\n",
        "        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\n",
        "        4. Separate users or seeds of items with ratings: item, user and rating columns provided\n",
        "        Args:\n",
        "            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\n",
        "            top_k (int): number of top items to recommend\n",
        "            sort_top_k (bool): flag to sort top k results\n",
        "        Returns:\n",
        "            pandas.DataFrame: sorted top k recommendation items\n",
        "        \"\"\"\n",
        "\n",
        "        # convert item ids to indices\n",
        "        item_ids = np.asarray(\n",
        "            list(\n",
        "                map(\n",
        "                    lambda item: self.item2index.get(item, np.NaN),\n",
        "                    items[self.col_item].values,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # if no ratings were provided assume they are all 1\n",
        "        if self.col_rating in items.columns:\n",
        "            ratings = items[self.col_rating]\n",
        "        else:\n",
        "            ratings = pd.Series(np.ones_like(item_ids))\n",
        "\n",
        "        # create local map of user ids\n",
        "        if self.col_user in items.columns:\n",
        "            test_users = items[self.col_user]\n",
        "            user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n",
        "            user_ids = test_users.map(user2index)\n",
        "        else:\n",
        "            # if no user column exists assume all entries are for a single user\n",
        "            test_users = pd.Series(np.zeros_like(item_ids))\n",
        "            user_ids = test_users\n",
        "        n_users = user_ids.drop_duplicates().shape[0]\n",
        "\n",
        "        # generate pseudo user affinity using seed items\n",
        "        pseudo_affinity = sparse.coo_matrix(\n",
        "            (ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)\n",
        "        ).tocsr()\n",
        "\n",
        "        # calculate raw scores with a matrix multiplication\n",
        "        test_scores = pseudo_affinity.dot(self.item_similarity)\n",
        "\n",
        "        # remove items in the seed set so recommended items are novel\n",
        "        test_scores[user_ids, item_ids] = -np.inf\n",
        "\n",
        "        top_items, top_scores = get_top_k_scored_items(\n",
        "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
        "        )\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                self.col_user: np.repeat(\n",
        "                    test_users.drop_duplicates().values, top_items.shape[1]\n",
        "                ),\n",
        "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
        "                self.col_prediction: top_scores.flatten(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # drop invalid items\n",
        "        return df.replace(-np.inf, np.nan).dropna()\n",
        "\n",
        "    def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n",
        "        \"\"\"Recommend top K items for all users which are in the test set\n",
        "        Args:\n",
        "            test (pandas.DataFrame): users to test\n",
        "            top_k (int): number of top items to recommend\n",
        "            sort_top_k (bool): flag to sort top k results\n",
        "            remove_seen (bool): flag to remove items seen in training from recommendation\n",
        "        Returns:\n",
        "            pandas.DataFrame: top k recommendation items for each user\n",
        "        \"\"\"\n",
        "\n",
        "        test_scores = self.score(test, remove_seen=remove_seen)\n",
        "\n",
        "        top_items, top_scores = get_top_k_scored_items(\n",
        "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
        "        )\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                self.col_user: np.repeat(\n",
        "                    test[self.col_user].drop_duplicates().values, top_items.shape[1]\n",
        "                ),\n",
        "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
        "                self.col_prediction: top_scores.flatten(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # drop invalid items\n",
        "        return df.replace(-np.inf, np.nan).dropna()\n",
        "\n",
        "    def predict(self, test):\n",
        "        \"\"\"Output SAR scores for only the users-items pairs which are in the test set\n",
        "        Args:\n",
        "            test (pandas.DataFrame): DataFrame that contains users and items to test\n",
        "        Returns:\n",
        "            pandas.DataFrame: DataFrame contains the prediction results\n",
        "        \"\"\"\n",
        "\n",
        "        test_scores = self.score(test)\n",
        "        user_ids = np.asarray(\n",
        "            list(\n",
        "                map(\n",
        "                    lambda user: self.user2index.get(user, np.NaN),\n",
        "                    test[self.col_user].values,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # create mapping of new items to zeros\n",
        "        item_ids = np.asarray(\n",
        "            list(\n",
        "                map(\n",
        "                    lambda item: self.item2index.get(item, np.NaN),\n",
        "                    test[self.col_item].values,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        nans = np.isnan(item_ids)\n",
        "        if any(nans):\n",
        "            logger.warning(\n",
        "                \"Items found in test not seen during training, new items will have score of 0\"\n",
        "            )\n",
        "            test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n",
        "            item_ids[nans] = self.n_items\n",
        "            item_ids = item_ids.astype(\"int64\")\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                self.col_user: test[self.col_user].values,\n",
        "                self.col_item: test[self.col_item].values,\n",
        "                self.col_prediction: test_scores[user_ids, item_ids],\n",
        "            }\n",
        "        )\n",
        "        return df"
      ],
      "metadata": {
        "id": "c7CUBSCEdE8h"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'"
      ],
      "metadata": {
        "id": "Dq1CqN2EeM-k"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_pandas_df(\n",
        "    size=MOVIELENS_DATA_SIZE,\n",
        "    header=['UserId', 'MovieId', 'Rating', 'Timestamp'],\n",
        "    title_col='Title'\n",
        ")\n",
        "\n",
        "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
        "data.loc[:, 'Rating'] = data['Rating'].astype(np.float32)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "8PCi8Eeee3Go",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "4539fdfe-6f70-4996-b63a-85578a9276c2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpshq_6hqn/ml-100k.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.81k/4.81k [00:01<00:00, 3.11kKB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   UserId  MovieId  Rating  Timestamp         Title\n",
              "0     196      242     3.0  881250949  Kolya (1996)\n",
              "1      63      242     3.0  875747190  Kolya (1996)\n",
              "2     226      242     5.0  883888671  Kolya (1996)\n",
              "3     154      242     3.0  879138235  Kolya (1996)\n",
              "4     306      242     5.0  876503793  Kolya (1996)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d32e1f56-5678-416d-81af-82a549b0d862\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>MovieId</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "      <td>Kolya (1996)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>875747190</td>\n",
              "      <td>Kolya (1996)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>226</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>883888671</td>\n",
              "      <td>Kolya (1996)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>879138235</td>\n",
              "      <td>Kolya (1996)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>306</td>\n",
              "      <td>242</td>\n",
              "      <td>5.0</td>\n",
              "      <td>876503793</td>\n",
              "      <td>Kolya (1996)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d32e1f56-5678-416d-81af-82a549b0d862')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d32e1f56-5678-416d-81af-82a549b0d862 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d32e1f56-5678-416d-81af-82a549b0d862');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    \"col_user\": \"UserId\",\n",
        "    \"col_item\": \"MovieId\",\n",
        "    \"col_rating\": \"Rating\",\n",
        "    \"col_timestamp\": \"Timestamp\",\n",
        "    \"col_prediction\": \"Prediction\",\n",
        "}"
      ],
      "metadata": {
        "id": "Eb-7qVBSkiMz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = python_stratified_split(data, ratio=0.75, col_user=header[\"col_user\"], col_item=header[\"col_item\"], seed=42)"
      ],
      "metadata": {
        "id": "Nwc59oSAlvyj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SARSingleNode(\n",
        "    similarity_type=\"jaccard\", \n",
        "    time_decay_coefficient=30, \n",
        "    time_now=None, \n",
        "    timedecay_formula=True, \n",
        "    **header\n",
        ")"
      ],
      "metadata": {
        "id": "DZyks1cgl0R2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train)"
      ],
      "metadata": {
        "id": "2Maj0eKKl7_l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = model.recommend_k_items(test, remove_seen=True)"
      ],
      "metadata": {
        "id": "zDx9ZTdkl-Nz"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_with_titles = (top_k.join(data[['MovieId', 'Title']].drop_duplicates().set_index('MovieId'), \n",
        "                                on='MovieId', \n",
        "                                how='inner').sort_values(by=['UserId', 'Prediction'], ascending=False))\n",
        "display(top_k_with_titles.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "l87sAvb9oSiE",
        "outputId": "dd668043-114c-438c-f98b-9990aadb6b27"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      UserId  MovieId  Prediction                              Title\n",
              "9420     943       82   21.313228               Jurassic Park (1993)\n",
              "9421     943      403   21.158839                      Batman (1989)\n",
              "9422     943      568   20.962922                       Speed (1994)\n",
              "9423     943      423   20.162170  E.T. the Extra-Terrestrial (1982)\n",
              "9424     943       89   19.890513                Blade Runner (1982)\n",
              "9425     943      393   19.832944              Mrs. Doubtfire (1993)\n",
              "9426     943       11   19.570244               Seven (Se7en) (1995)\n",
              "9427     943       71   19.553877              Lion King, The (1994)\n",
              "9428     943      202   19.422129               Groundhog Day (1993)\n",
              "9429     943      238   19.115604             Raising Arizona (1987)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40df7685-bc32-4dc7-ba54-bb689d7e15ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>MovieId</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9420</th>\n",
              "      <td>943</td>\n",
              "      <td>82</td>\n",
              "      <td>21.313228</td>\n",
              "      <td>Jurassic Park (1993)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9421</th>\n",
              "      <td>943</td>\n",
              "      <td>403</td>\n",
              "      <td>21.158839</td>\n",
              "      <td>Batman (1989)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9422</th>\n",
              "      <td>943</td>\n",
              "      <td>568</td>\n",
              "      <td>20.962922</td>\n",
              "      <td>Speed (1994)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9423</th>\n",
              "      <td>943</td>\n",
              "      <td>423</td>\n",
              "      <td>20.162170</td>\n",
              "      <td>E.T. the Extra-Terrestrial (1982)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9424</th>\n",
              "      <td>943</td>\n",
              "      <td>89</td>\n",
              "      <td>19.890513</td>\n",
              "      <td>Blade Runner (1982)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9425</th>\n",
              "      <td>943</td>\n",
              "      <td>393</td>\n",
              "      <td>19.832944</td>\n",
              "      <td>Mrs. Doubtfire (1993)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9426</th>\n",
              "      <td>943</td>\n",
              "      <td>11</td>\n",
              "      <td>19.570244</td>\n",
              "      <td>Seven (Se7en) (1995)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9427</th>\n",
              "      <td>943</td>\n",
              "      <td>71</td>\n",
              "      <td>19.553877</td>\n",
              "      <td>Lion King, The (1994)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9428</th>\n",
              "      <td>943</td>\n",
              "      <td>202</td>\n",
              "      <td>19.422129</td>\n",
              "      <td>Groundhog Day (1993)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9429</th>\n",
              "      <td>943</td>\n",
              "      <td>238</td>\n",
              "      <td>19.115604</td>\n",
              "      <td>Raising Arizona (1987)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40df7685-bc32-4dc7-ba54-bb689d7e15ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40df7685-bc32-4dc7-ba54-bb689d7e15ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40df7685-bc32-4dc7-ba54-bb689d7e15ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "04Q5wMi1u6p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all ranking metrics have the same arguments\n",
        "args = [test, top_k]\n",
        "kwargs = dict(col_user='UserId', \n",
        "              col_item='MovieId', \n",
        "              col_rating='Rating', \n",
        "              col_prediction='Prediction', \n",
        "              relevancy_method='top_k', \n",
        "              k=TOP_K)\n",
        "\n",
        "eval_map = map_at_k(*args, **kwargs)\n",
        "eval_ndcg = ndcg_at_k(*args, **kwargs)\n",
        "eval_precision = precision_at_k(*args, **kwargs)\n",
        "eval_recall = recall_at_k(*args, **kwargs)"
      ],
      "metadata": {
        "id": "hx9F4smEobua"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model:\",\n",
        "      f\"Top K:\\t\\t {TOP_K}\",\n",
        "      f\"MAP:\\t\\t {eval_map:f}\",\n",
        "      f\"NDCG:\\t\\t {eval_ndcg:f}\",\n",
        "      f\"Precision@K:\\t {eval_precision:f}\",\n",
        "      f\"Recall@K:\\t {eval_recall:f}\", sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WCfRzBHvB3t",
        "outputId": "3f80b780-0b25-46c4-fac1-5dbe7100249f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\n",
            "Top K:\t\t 10\n",
            "MAP:\t\t 0.095544\n",
            "NDCG:\t\t 0.350232\n",
            "Precision@K:\t 0.305726\n",
            "Recall@K:\t 0.164690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0AJ0rE_kvDmK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}