{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zE2TKGeZ_F5Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as sk_split\n",
    "import math\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBL39-epcnGF"
   },
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FF9SNnDMc_IH"
   },
   "outputs": [],
   "source": [
    "DEFAULT_USER_COL = \"userID\"\n",
    "DEFAULT_ITEM_COL = \"itemID\"\n",
    "DEFAULT_RATING_COL = \"rating\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_GENRE_COL = \"genre\"\n",
    "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_SIMILARITY_COL = \"sim\"\n",
    "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
    "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\"\n",
    "\n",
    "DEFAULT_HEADER = (\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ITEM_COL,\n",
    "    DEFAULT_RATING_COL,\n",
    "    DEFAULT_TIMESTAMP_COL,\n",
    ")\n",
    "\n",
    "COL_DICT = {\n",
    "    \"col_user\": DEFAULT_USER_COL,\n",
    "    \"col_item\": DEFAULT_ITEM_COL,\n",
    "    \"col_rating\": DEFAULT_RATING_COL,\n",
    "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
    "}\n",
    "\n",
    "# Filtering variables\n",
    "DEFAULT_K = 10\n",
    "DEFAULT_THRESHOLD = 10\n",
    "\n",
    "# Other\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "TppmtJ7zrKcf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        shell_name = get_ipython().__class__.__name__\n",
    "        if shell_name == \"ZMQInteractiveShell\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_databricks():\n",
    "    try:\n",
    "        if os.path.realpath(\".\") == \"/databricks/driver\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "E0w_zE5XrXKa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import math\n",
    "import zipfile\n",
    "from contextlib import contextmanager\n",
    "from tempfile import TemporaryDirectory\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def maybe_download(url, filename=None, work_directory=\".\", expected_bytes=None):\n",
    "    if filename is None:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "    os.makedirs(work_directory, exist_ok=True)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    print(filepath)\n",
    "    if not os.path.exists(filepath):\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            log.info(f\"Downloading {url}\")\n",
    "            total_size = int(r.headers.get(\"content-length\", 0))\n",
    "            block_size = 1024\n",
    "            num_iterables = math.ceil(total_size / block_size)\n",
    "            with open(filepath, \"wb\") as file:\n",
    "                for data in tqdm(\n",
    "                    r.iter_content(block_size),\n",
    "                    total=num_iterables,\n",
    "                    unit=\"KB\",\n",
    "                    unit_scale=True,\n",
    "                ):\n",
    "                    file.write(data)\n",
    "        else:\n",
    "            log.error(f\"Problem downloading {url}\")\n",
    "            r.raise_for_status()\n",
    "    else:\n",
    "        log.info(f\"File {filepath} already downloaded\")\n",
    "        print(\"File Found\")\n",
    "    if expected_bytes is not None:\n",
    "        statinfo = os.stat(filepath)\n",
    "        if statinfo.st_size != expected_bytes:\n",
    "            os.remove(filepath)\n",
    "            raise IOError(f\"Failed to verify {filepath}\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def download_path(path=None):\n",
    "    if path is None:\n",
    "        tmp_dir = TemporaryDirectory()\n",
    "        try:\n",
    "            yield tmp_dir.name\n",
    "        finally:\n",
    "            tmp_dir.cleanup()\n",
    "    else:\n",
    "        path = os.path.realpath(path)\n",
    "        yield path\n",
    "\n",
    "\n",
    "def unzip_file(zip_src, dst_dir, clean_zip_file=False):\n",
    "    fz = zipfile.ZipFile(zip_src, \"r\")\n",
    "    for file in fz.namelist():\n",
    "        fz.extract(file, dst_dir)\n",
    "    if clean_zip_file:\n",
    "        os.remove(zip_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "e9Ae__iItWnN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from zipfile import ZipFile\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.types import (\n",
    "        StructType,\n",
    "        StructField,\n",
    "        StringType,\n",
    "        IntegerType,\n",
    "        FloatType,\n",
    "        LongType,\n",
    "    )\n",
    "except ImportError:\n",
    "    pass  \n",
    "\n",
    "\n",
    "class _DataFormat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sep,\n",
    "        path,\n",
    "        has_header=False,\n",
    "        item_sep=None,\n",
    "        item_path=None,\n",
    "        item_has_header=False,\n",
    "    ):\n",
    "\n",
    "\n",
    "        self._sep = sep\n",
    "        self._path = path\n",
    "        self._has_header = has_header\n",
    "\n",
    "        self._item_sep = item_sep\n",
    "        self._item_path = item_path\n",
    "        self._item_has_header = item_has_header\n",
    "\n",
    "    @property\n",
    "    def separator(self):\n",
    "        return self._sep\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def has_header(self):\n",
    "        return self._has_header\n",
    "\n",
    "    @property\n",
    "    def item_separator(self):\n",
    "        return self._item_sep\n",
    "\n",
    "    @property\n",
    "    def item_path(self):\n",
    "        return self._item_path\n",
    "\n",
    "    @property\n",
    "    def item_has_header(self):\n",
    "        return self._item_has_header\n",
    "\n",
    "DATA_FORMAT = {\n",
    "    \"100k\": _DataFormat(\"\\t\", \"ml-100k/u.data\", False, \"|\", \"ml-100k/u.item\", False),\n",
    "    \"1m\": _DataFormat(\n",
    "        \"::\", \"ml-1m/ratings.dat\", False, \"::\", \"ml-1m/movies.dat\", False\n",
    "    ),\n",
    "    \"10m\": _DataFormat(\n",
    "        \"::\", \"ml-10M100K/ratings.dat\", False, \"::\", \"ml-10M100K/movies.dat\", False\n",
    "    ),\n",
    "    \"20m\": _DataFormat(\",\", \"ml-20m/ratings.csv\", True, \",\", \"ml-20m/movies.csv\", True),\n",
    "}\n",
    "\n",
    "MOCK_DATA_FORMAT = {\n",
    "    \"mock100\": {\"size\": 100, \"seed\": 6},\n",
    "}\n",
    "\n",
    "\n",
    "GENRES = (\n",
    "    \"unknown\",\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children's\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "WARNING_MOVIE_LENS_HEADER = \"\"\"MovieLens rating dataset has four columns\n",
    "    (user id, movie id, rating, and timestamp), but more than four column names are provided.\n",
    "    Will only use the first four column names.\"\"\"\n",
    "WARNING_HAVE_SCHEMA_AND_HEADER = \"\"\"Both schema and header are provided.\n",
    "    The header argument will be ignored.\"\"\"\n",
    "ERROR_MOVIE_LENS_SIZE = (\n",
    "    \"Invalid data size. Should be one of {100k, 1m, 10m, or 20m, or mock100}\"\n",
    ")\n",
    "ERROR_HEADER = \"Header error. At least user and movie column names should be provided\"\n",
    "\n",
    "\n",
    "def load_pandas_df(\n",
    "    size=\"100k\",\n",
    "    header=None,\n",
    "    local_cache_path=None,\n",
    "    title_col=None,\n",
    "    genres_col=None,\n",
    "    year_col=None,\n",
    "):\n",
    "\n",
    "    size = size.lower()\n",
    "    if size not in DATA_FORMAT and size not in MOCK_DATA_FORMAT:\n",
    "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
    "\n",
    "    if header is None:\n",
    "        header = DEFAULT_HEADER\n",
    "    elif len(header) < 2:\n",
    "        raise ValueError(ERROR_HEADER)\n",
    "    elif len(header) > 4:\n",
    "        warnings.warn(WARNING_MOVIE_LENS_HEADER)\n",
    "        header = header[:4]\n",
    "\n",
    "    if size in MOCK_DATA_FORMAT:\n",
    "        # generate fake data\n",
    "        return MockMovielensSchema.get_df(\n",
    "            keep_first_n_cols=len(header),\n",
    "            keep_title_col=(title_col is not None),\n",
    "            keep_genre_col=(genres_col is not None),\n",
    "            **MOCK_DATA_FORMAT[\n",
    "                size\n",
    "            ], \n",
    "        )\n",
    "\n",
    "    movie_col = header[1]\n",
    "\n",
    "    with download_path(local_cache_path) as path:\n",
    "        filepath = os.path.join(path, \"ml-{}.zip\".format(size))\n",
    "\n",
    "        datapath, item_datapath = _maybe_download_and_extract(size, filepath)\n",
    "\n",
    "        item_df = _load_item_df(\n",
    "            size, item_datapath, movie_col, title_col, genres_col, year_col\n",
    "        )\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            datapath,\n",
    "            sep=DATA_FORMAT[size].separator,\n",
    "            engine=\"python\",\n",
    "            names=header,\n",
    "            usecols=[*range(len(header))],\n",
    "            header=0 if DATA_FORMAT[size].has_header else None,\n",
    "        )\n",
    "\n",
    "        if len(header) > 2:\n",
    "            df[header[2]] = df[header[2]].astype(float)\n",
    "\n",
    "        if item_df is not None:\n",
    "            df = df.merge(item_df, on=header[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_item_df(\n",
    "    size=\"100k\",\n",
    "    local_cache_path=None,\n",
    "    movie_col=DEFAULT_ITEM_COL,\n",
    "    title_col=None,\n",
    "    genres_col=None,\n",
    "    year_col=None,\n",
    "):\n",
    "\n",
    "    size = size.lower()\n",
    "    if size not in DATA_FORMAT:\n",
    "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
    "\n",
    "    with download_path(local_cache_path) as path:\n",
    "        filepath = os.path.join(path, \"ml-{}.zip\".format(size))\n",
    "        _, item_datapath = _maybe_download_and_extract(size, filepath)\n",
    "        item_df = _load_item_df(\n",
    "            size, item_datapath, movie_col, title_col, genres_col, year_col\n",
    "        )\n",
    "\n",
    "    return item_df\n",
    "\n",
    "\n",
    "def _load_item_df(size, item_datapath, movie_col, title_col, genres_col, year_col):\n",
    "    \"\"\"Loads Movie info\"\"\"\n",
    "    if title_col is None and genres_col is None and year_col is None:\n",
    "        return None\n",
    "\n",
    "    item_header = [movie_col]\n",
    "    usecols = [0]\n",
    "\n",
    "    if title_col is not None or year_col is not None:\n",
    "        item_header.append(\"title_year\")\n",
    "        usecols.append(1)\n",
    "\n",
    "    genres_header_100k = None\n",
    "    if genres_col is not None:\n",
    "\n",
    "        if size == \"100k\":\n",
    "            genres_header_100k = [*(str(i) for i in range(19))]\n",
    "            item_header.extend(genres_header_100k)\n",
    "            usecols.extend([*range(5, 24)])  \n",
    "        else:\n",
    "            item_header.append(genres_col)\n",
    "            usecols.append(2) \n",
    "\n",
    "    item_df = pd.read_csv(\n",
    "        item_datapath,\n",
    "        sep=DATA_FORMAT[size].item_separator,\n",
    "        engine=\"python\",\n",
    "        names=item_header,\n",
    "        usecols=usecols,\n",
    "        header=0 if DATA_FORMAT[size].item_has_header else None,\n",
    "        encoding=\"ISO-8859-1\",\n",
    "    )\n",
    "\n",
    "    if genres_header_100k is not None:\n",
    "        item_df[genres_col] = item_df[genres_header_100k].values.tolist()\n",
    "        item_df[genres_col] = item_df[genres_col].map(\n",
    "            lambda l: \"|\".join([GENRES[i] for i, v in enumerate(l) if v == 1])\n",
    "        )\n",
    "\n",
    "        item_df.drop(genres_header_100k, axis=1, inplace=True)\n",
    "\n",
    "    if year_col is not None:\n",
    "\n",
    "        def parse_year(t):\n",
    "            parsed = re.split(\"[()]\", t)\n",
    "            if len(parsed) > 2 and parsed[-2].isdecimal():\n",
    "                return parsed[-2]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        item_df[year_col] = item_df[\"title_year\"].map(parse_year)\n",
    "        if title_col is None:\n",
    "            item_df.drop(\"title_year\", axis=1, inplace=True)\n",
    "\n",
    "    if title_col is not None:\n",
    "        item_df.rename(columns={\"title_year\": title_col}, inplace=True)\n",
    "\n",
    "    return item_df\n",
    "\n",
    "\n",
    "def _maybe_download_and_extract(size, dest_path):\n",
    "    dirs, _ = os.path.split(dest_path)\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    _, rating_filename = os.path.split(DATA_FORMAT[size].path)\n",
    "    rating_path = os.path.join(dirs, rating_filename)\n",
    "    _, item_filename = os.path.split(DATA_FORMAT[size].item_path)\n",
    "    item_path = os.path.join(dirs, item_filename)\n",
    "\n",
    "    if not os.path.exists(rating_path) or not os.path.exists(item_path):\n",
    "        download_movielens(size, dest_path)\n",
    "        extract_movielens(size, rating_path, item_path, dest_path)\n",
    "\n",
    "    return rating_path, item_path\n",
    "\n",
    "\n",
    "def download_movielens(size, dest_path):\n",
    "    if size not in DATA_FORMAT:\n",
    "        raise ValueError(ERROR_MOVIE_LENS_SIZE)\n",
    "\n",
    "    url = \"https://files.grouplens.org/datasets/movielens/ml-\" + size + \".zip\"\n",
    "    dirs, file = os.path.split(dest_path)\n",
    "    maybe_download(url, file, work_directory=dirs)\n",
    "\n",
    "\n",
    "def extract_movielens(size, rating_path, item_path, zip_path):\n",
    "    with ZipFile(zip_path, \"r\") as z:\n",
    "        with z.open(DATA_FORMAT[size].path) as zf, open(rating_path, \"wb\") as f:\n",
    "            shutil.copyfileobj(zf, f)\n",
    "        with z.open(DATA_FORMAT[size].item_path) as zf, open(item_path, \"wb\") as f:\n",
    "            shutil.copyfileobj(zf, f)\n",
    "\n",
    "def unique_columns(df, *, columns):\n",
    "    return not df[columns].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8ZG1UV1-oED9"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def exponential_decay(value, max_val, half_life):\n",
    "\n",
    "    return np.minimum(1.0, np.power(0.5, (max_val - value) / half_life))\n",
    "\n",
    "\n",
    "def jaccard(cooccurrence):\n",
    "\n",
    "    diag = cooccurrence.diagonal()\n",
    "    diag_rows = np.expand_dims(diag, axis=0)\n",
    "    diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        result = cooccurrence / (diag_rows + diag_cols - cooccurrence)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def lift(cooccurrence):\n",
    "\n",
    "    diag = cooccurrence.diagonal()\n",
    "    diag_rows = np.expand_dims(diag, axis=0)\n",
    "    diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        result = cooccurrence / (diag_rows * diag_cols)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def get_top_k_scored_items(scores, top_k, sort_top_k=False):\n",
    "\n",
    "    if isinstance(scores, sparse.spmatrix):\n",
    "        scores = scores.todense()\n",
    "\n",
    "    if scores.shape[1] < top_k:\n",
    "        logger.warning(\n",
    "            \"Number of items is less than top_k, limiting top_k to number of items\"\n",
    "        )\n",
    "    k = min(top_k, scores.shape[1])\n",
    "\n",
    "    test_user_idx = np.arange(scores.shape[0])[:, None]\n",
    "\n",
    "    top_items = np.argpartition(scores, -k, axis=1)[:, -k:]\n",
    "    top_scores = scores[test_user_idx, top_items]\n",
    "\n",
    "    if sort_top_k:\n",
    "        sort_ind = np.argsort(-top_scores)\n",
    "        top_items = top_items[test_user_idx, sort_ind]\n",
    "        top_scores = top_scores[test_user_idx, sort_ind]\n",
    "\n",
    "    return np.array(top_items), np.array(top_scores)\n",
    "\n",
    "\n",
    "def binarize(a, threshold):\n",
    "\n",
    "    return np.where(a > threshold, 1.0, 0.0)\n",
    "\n",
    "\n",
    "def rescale(data, new_min=0, new_max=1, data_min=None, data_max=None):\n",
    "\n",
    "    data_min = data.min() if data_min is None else data_min\n",
    "    data_max = data.max() if data_max is None else data_max\n",
    "    return (data - data_min) / (data_max - data_min) * (new_max - new_min) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LuZYkkrYcoua"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import lru_cache, wraps\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def user_item_pairs(\n",
    "    user_df,\n",
    "    item_df,\n",
    "    user_col=DEFAULT_USER_COL,\n",
    "    item_col=DEFAULT_ITEM_COL,\n",
    "    user_item_filter_df=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "):\n",
    "\n",
    "    user_df[\"key\"] = 1\n",
    "    item_df[\"key\"] = 1\n",
    "    users_items = user_df.merge(item_df, on=\"key\")\n",
    "\n",
    "    user_df.drop(\"key\", axis=1, inplace=True)\n",
    "    item_df.drop(\"key\", axis=1, inplace=True)\n",
    "    users_items.drop(\"key\", axis=1, inplace=True)\n",
    "\n",
    "    if user_item_filter_df is not None:\n",
    "        users_items = filter_by(users_items, user_item_filter_df, [user_col, item_col])\n",
    "\n",
    "    if shuffle:\n",
    "        users_items = users_items.sample(frac=1, random_state=seed).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    return users_items\n",
    "\n",
    "\n",
    "def filter_by(df, filter_by_df, filter_by_cols):\n",
    "\n",
    "\n",
    "    return df.loc[\n",
    "        ~df.set_index(filter_by_cols).index.isin(\n",
    "            filter_by_df.set_index(filter_by_cols).index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "class LibffmConverter:\n",
    "\n",
    "    def __init__(self, filepath=None):\n",
    "        self.filepath = filepath\n",
    "        self.col_rating = None\n",
    "        self.field_names = None\n",
    "        self.field_count = None\n",
    "        self.feature_count = None\n",
    "\n",
    "    def fit(self, df, col_rating=DEFAULT_RATING_COL):\n",
    "\n",
    "        types = df.dtypes\n",
    "        if not all(\n",
    "            [\n",
    "                x == object or np.issubdtype(x, np.integer) or x == np.float\n",
    "                for x in types\n",
    "            ]\n",
    "        ):\n",
    "            raise TypeError(\"Input columns should be only object and/or numeric types.\")\n",
    "\n",
    "        if col_rating not in df.columns:\n",
    "            raise TypeError(\n",
    "                \"Column of {} is not in input dataframe columns\".format(col_rating)\n",
    "            )\n",
    "\n",
    "        self.col_rating = col_rating\n",
    "        self.field_names = list(df.drop(col_rating, axis=1).columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Tranform an input dataset with the same schema (column names and dtypes) to libffm format\n",
    "        by using the fitted converter.\n",
    "        Args:\n",
    "            df (pandas.DataFrame): input Pandas dataframe.\n",
    "        Return:\n",
    "            pandas.DataFrame: Output libffm format dataframe.\n",
    "        \"\"\"\n",
    "        if self.col_rating not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"Input dataset does not contain the label column {} in the fitting dataset\".format(\n",
    "                    self.col_rating\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if not all([x in df.columns for x in self.field_names]):\n",
    "            raise ValueError(\n",
    "                \"Not all columns in the input dataset appear in the fitting dataset\"\n",
    "            )\n",
    "\n",
    "        idx = 1\n",
    "        self.field_feature_dict = {}\n",
    "        for field in self.field_names:\n",
    "            for feature in df[field].values:\n",
    "\n",
    "                if (field, feature) not in self.field_feature_dict:\n",
    "                    self.field_feature_dict[(field, feature)] = idx\n",
    "                    if df[field].dtype == object:\n",
    "                        idx += 1\n",
    "            if df[field].dtype != object:\n",
    "                idx += 1\n",
    "\n",
    "        self.field_count = len(self.field_names)\n",
    "        self.feature_count = idx - 1\n",
    "\n",
    "        def _convert(field, feature, field_index, field_feature_index_dict):\n",
    "            field_feature_index = field_feature_index_dict[(field, feature)]\n",
    "            if isinstance(feature, str):\n",
    "                feature = 1\n",
    "            return \"{}:{}:{}\".format(field_index, field_feature_index, feature)\n",
    "\n",
    "        for col_index, col in enumerate(self.field_names):\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: _convert(col, x, col_index + 1, self.field_feature_dict)\n",
    "            )\n",
    "\n",
    "        column_names = self.field_names[:]\n",
    "        column_names.insert(0, self.col_rating)\n",
    "        df = df[column_names]\n",
    "\n",
    "        if self.filepath is not None:\n",
    "            np.savetxt(self.filepath, df.values, delimiter=\" \", fmt=\"%s\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df, col_rating=DEFAULT_RATING_COL):\n",
    "\n",
    "        return self.fit(df, col_rating=col_rating).transform(df)\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return {\n",
    "            \"field count\": self.field_count,\n",
    "            \"feature count\": self.feature_count,\n",
    "            \"file path\": self.filepath,\n",
    "        }\n",
    "\n",
    "\n",
    "def negative_feedback_sampler(\n",
    "    df,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_label=DEFAULT_LABEL_COL,\n",
    "    col_feedback=\"feedback\",\n",
    "    ratio_neg_per_user=1,\n",
    "    pos_value=1,\n",
    "    neg_value=0,\n",
    "    seed=42,\n",
    "):\n",
    "\n",
    "    items = df[col_item].unique()\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def sample_items(user_df):\n",
    "        n_u = len(user_df)\n",
    "        neg_sample_size = max(round(n_u * ratio_neg_per_user), 1)\n",
    "\n",
    "        sample_size = min(n_u + neg_sample_size, len(items))\n",
    "        items_sample = rng.choice(items, sample_size, replace=False)\n",
    "        new_items = np.setdiff1d(items_sample, user_df[col_item])[:neg_sample_size]\n",
    "        new_df = pd.DataFrame(\n",
    "            data={\n",
    "                col_user: user_df.name,\n",
    "                col_item: new_items,\n",
    "                col_label: neg_value,\n",
    "            }\n",
    "        )\n",
    "        return pd.concat([user_df, new_df], ignore_index=True)\n",
    "\n",
    "    res_df = df.copy()\n",
    "    res_df[col_label] = pos_value\n",
    "    return (\n",
    "        res_df.groupby(col_user)\n",
    "        .apply(sample_items)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={col_label: col_feedback})\n",
    "    )\n",
    "\n",
    "\n",
    "def has_columns(df, columns):\n",
    "\n",
    "\n",
    "    result = True\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            logger.error(\"Missing column: {} in DataFrame\".format(column))\n",
    "            result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def has_same_base_dtype(df_1, df_2, columns=None):\n",
    "\n",
    "    if columns is None:\n",
    "        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):\n",
    "            logger.error(\n",
    "                \"Cannot test all columns because they are not all shared across DataFrames\"\n",
    "            )\n",
    "            return False\n",
    "        columns = df_1.columns\n",
    "\n",
    "    if not (\n",
    "        has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    result = True\n",
    "    for column in columns:\n",
    "        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:\n",
    "            logger.error(\"Columns {} do not have the same base datatype\".format(column))\n",
    "            result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class PandasHash:\n",
    "\n",
    "    __slots__ = \"pandas_object\"\n",
    "\n",
    "    def __init__(self, pandas_object):\n",
    "\n",
    "        if not isinstance(pandas_object, (pd.DataFrame, pd.Series)):\n",
    "            raise TypeError(\"Can only wrap pandas DataFrame or Series objects\")\n",
    "        self.pandas_object = pandas_object\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        hashable = tuple(self.pandas_object.values.tobytes())\n",
    "        if isinstance(self.pandas_object, pd.DataFrame):\n",
    "            hashable += tuple(self.pandas_object.columns)\n",
    "        else:\n",
    "            hashable += tuple(self.pandas_object.name)\n",
    "        return hash(hashable)\n",
    "\n",
    "\n",
    "def lru_cache_df(maxsize, typed=False):\n",
    "\n",
    "    def to_pandas_hash(val):\n",
    "        return PandasHash(val) if isinstance(val, pd.DataFrame) else val\n",
    "\n",
    "    def from_pandas_hash(val):\n",
    "        return val.pandas_object if isinstance(val, PandasHash) else val\n",
    "\n",
    "    def decorating_function(user_function):\n",
    "        @wraps(user_function)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            args = tuple([to_pandas_hash(a) for a in args])\n",
    "            kwargs = {k: to_pandas_hash(v) for k, v in kwargs.items()}\n",
    "            return cached_wrapper(*args, **kwargs)\n",
    "\n",
    "        @lru_cache(maxsize=maxsize, typed=typed)\n",
    "        def cached_wrapper(*args, **kwargs):\n",
    "            args = tuple([from_pandas_hash(a) for a in args])\n",
    "            kwargs = {k: from_pandas_hash(v) for k, v in kwargs.items()}\n",
    "            return user_function(*args, **kwargs)\n",
    "\n",
    "        wrapper.cache_info = cached_wrapper.cache_info\n",
    "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorating_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QYwowadYVtY6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import functions as F, Window\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def process_split_ratio(ratio):\n",
    "\n",
    "    if isinstance(ratio, float):\n",
    "        if ratio <= 0 or ratio >= 1:\n",
    "            raise ValueError(\"Split ratio has to be between 0 and 1\")\n",
    "\n",
    "        multi = False\n",
    "    elif isinstance(ratio, list):\n",
    "        if any([x <= 0 for x in ratio]):\n",
    "            raise ValueError(\n",
    "                \"All split ratios in the ratio list should be larger than 0.\"\n",
    "            )\n",
    "\n",
    "        if math.fsum(ratio) != 1.0:\n",
    "            ratio = [x / math.fsum(ratio) for x in ratio]\n",
    "\n",
    "        multi = True\n",
    "    else:\n",
    "        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n",
    "\n",
    "    return multi, ratio\n",
    "\n",
    "\n",
    "def min_rating_filter_pandas(\n",
    "    data,\n",
    "    min_rating=1,\n",
    "    filter_by=\"user\",\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "):\n",
    "\n",
    "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
    "\n",
    "    if min_rating < 1:\n",
    "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
    "\n",
    "    return data.groupby(split_by_column).filter(lambda x: len(x) >= min_rating)\n",
    "\n",
    "\n",
    "def min_rating_filter_spark(\n",
    "    data,\n",
    "    min_rating=1,\n",
    "    filter_by=\"user\",\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "):\n",
    "\n",
    "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
    "\n",
    "    if min_rating < 1:\n",
    "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
    "\n",
    "    if min_rating > 1:\n",
    "        window = Window.partitionBy(split_by_column)\n",
    "        data = (\n",
    "            data.withColumn(\"_count\", F.count(split_by_column).over(window))\n",
    "            .where(F.col(\"_count\") >= min_rating)\n",
    "            .drop(\"_count\")\n",
    "        )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _get_column_name(name, col_user, col_item):\n",
    "    if name == \"user\":\n",
    "        return col_user\n",
    "    elif name == \"item\":\n",
    "        return col_item\n",
    "    else:\n",
    "        raise ValueError(\"name should be either 'user' or 'item'.\")\n",
    "\n",
    "\n",
    "def split_pandas_data_with_ratios(data, ratios, seed=42, shuffle=False):\n",
    "\n",
    "    if math.fsum(ratios) != 1.0:\n",
    "        raise ValueError(\"The ratios have to sum to 1\")\n",
    "\n",
    "    split_index = np.cumsum(ratios).tolist()[:-1]\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1, random_state=seed)\n",
    "\n",
    "    splits = np.split(data, [round(x * len(data)) for x in split_index])\n",
    "\n",
    "    for i in range(len(ratios)):\n",
    "        splits[i][\"split_index\"] = i\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "def filter_k_core(data, core_num=0, col_user=\"userID\", col_item=\"itemID\"):\n",
    "\n",
    "    num_users, num_items = len(data[col_user].unique()), len(data[col_item].unique())\n",
    "    logger.info(\"Original: %d users and %d items\", num_users, num_items)\n",
    "    df_inp = data.copy()\n",
    "\n",
    "    if core_num > 0:\n",
    "        while True:\n",
    "            df_inp = min_rating_filter_pandas(\n",
    "                df_inp, min_rating=core_num, filter_by=\"item\"\n",
    "            )\n",
    "            df_inp = min_rating_filter_pandas(\n",
    "                df_inp, min_rating=core_num, filter_by=\"user\"\n",
    "            )\n",
    "            count_u = df_inp.groupby(col_user)[col_item].count()\n",
    "            count_i = df_inp.groupby(col_item)[col_user].count()\n",
    "            if (\n",
    "                len(count_i[count_i < core_num]) == 0\n",
    "                and len(count_u[count_u < core_num]) == 0\n",
    "            ):\n",
    "                break\n",
    "    df_inp = df_inp.sort_values(by=[col_user])\n",
    "    num_users = len(df_inp[col_user].unique())\n",
    "    num_items = len(df_inp[col_item].unique())\n",
    "    logger.info(\"Final: %d users and %d items\", num_users, num_items)\n",
    "\n",
    "    return df_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "byq7NHDJUsMV"
   },
   "outputs": [],
   "source": [
    "def _do_stratification(\n",
    "    data,\n",
    "    ratio=0.8,\n",
    "    min_rating=1,\n",
    "    filter_by=\"user\",\n",
    "    is_random=True,\n",
    "    seed=42,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
    "):\n",
    "    if not (filter_by == \"user\" or filter_by == \"item\"):\n",
    "        raise ValueError(\"filter_by should be either 'user' or 'item'.\")\n",
    "\n",
    "    if min_rating < 1:\n",
    "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
    "\n",
    "    if col_user not in data.columns:\n",
    "        raise ValueError(\"Schema of data not valid. Missing User Col\")\n",
    "\n",
    "    if col_item not in data.columns:\n",
    "        raise ValueError(\"Schema of data not valid. Missing Item Col\")\n",
    "\n",
    "    if not is_random:\n",
    "        if col_timestamp not in data.columns:\n",
    "            raise ValueError(\"Schema of data not valid. Missing Timestamp Col\")\n",
    "\n",
    "    multi_split, ratio = process_split_ratio(ratio)\n",
    "\n",
    "    split_by_column = col_user if filter_by == \"user\" else col_item\n",
    "\n",
    "    ratio = ratio if multi_split else [ratio, 1 - ratio]\n",
    "\n",
    "    if min_rating > 1:\n",
    "        data = min_rating_filter_pandas(\n",
    "            data,\n",
    "            min_rating=min_rating,\n",
    "            filter_by=filter_by,\n",
    "            col_user=col_user,\n",
    "            col_item=col_item,\n",
    "        )\n",
    "\n",
    "    splits = []\n",
    "\n",
    "    df_grouped = (\n",
    "        data.sort_values(col_timestamp).groupby(split_by_column)\n",
    "        if is_random is False\n",
    "        else data.groupby(split_by_column)\n",
    "    )\n",
    "\n",
    "    for _, group in df_grouped:\n",
    "        group_splits = split_pandas_data_with_ratios(\n",
    "            group, ratio, shuffle=is_random, seed=seed\n",
    "        )\n",
    "\n",
    "        concat_group_splits = pd.concat(group_splits)\n",
    "\n",
    "        splits.append(concat_group_splits)\n",
    "\n",
    "    splits_all = pd.concat(splits)\n",
    "\n",
    "    splits_list = [\n",
    "        splits_all[splits_all[\"split_index\"] == x].drop(\"split_index\", axis=1)\n",
    "        for x in range(len(ratio))\n",
    "    ]\n",
    "\n",
    "    return splits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "hez73jENTKLN"
   },
   "outputs": [],
   "source": [
    "def python_stratified_split(\n",
    "    data,\n",
    "    ratio=0.75,\n",
    "    min_rating=1,\n",
    "    filter_by=\"user\",\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    seed=42,\n",
    "):\n",
    "\n",
    "    return _do_stratification(\n",
    "        data,\n",
    "        ratio=ratio,\n",
    "        min_rating=min_rating,\n",
    "        filter_by=filter_by,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        is_random=True,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v_PXSVEWONp"
   },
   "source": [
    "Evaluation Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "U4iY48VtcBcI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "\n",
    "def _check_column_dtypes(func):\n",
    "\n",
    "    @wraps(func)\n",
    "    def check_column_dtypes_wrapper(\n",
    "        rating_true,\n",
    "        rating_pred,\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        col_rating=DEFAULT_RATING_COL,\n",
    "        col_prediction=DEFAULT_PREDICTION_COL,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "\n",
    "        if not has_columns(rating_true, [col_user, col_item, col_rating]):\n",
    "            raise ValueError(\"Missing columns in true rating DataFrame\")\n",
    "        if not has_columns(rating_pred, [col_user, col_item, col_prediction]):\n",
    "            raise ValueError(\"Missing columns in predicted rating DataFrame\")\n",
    "        if not has_same_base_dtype(\n",
    "            rating_true, rating_pred, columns=[col_user, col_item]\n",
    "        ):\n",
    "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
    "\n",
    "        return func(\n",
    "            rating_true=rating_true,\n",
    "            rating_pred=rating_pred,\n",
    "            col_user=col_user,\n",
    "            col_item=col_item,\n",
    "            col_rating=col_rating,\n",
    "            col_prediction=col_prediction,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return check_column_dtypes_wrapper\n",
    "\n",
    "\n",
    "@_check_column_dtypes\n",
    "@lru_cache_df(maxsize=1)\n",
    "def merge_rating_true_pred(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    suffixes = [\"_true\", \"_pred\"]\n",
    "    rating_true_pred = pd.merge(\n",
    "        rating_true, rating_pred, on=[col_user, col_item], suffixes=suffixes\n",
    "    )\n",
    "    if col_rating in rating_pred.columns:\n",
    "        col_rating = col_rating + suffixes[0]\n",
    "    if col_prediction in rating_true.columns:\n",
    "        col_prediction = col_prediction + suffixes[1]\n",
    "    return rating_true_pred[col_rating], rating_true_pred[col_prediction]\n",
    "\n",
    "\n",
    "def rmse(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def mae(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "def rsquared(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def exp_var(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return explained_variance_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def auc(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def logloss(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    y_true, y_pred = merge_rating_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "    )\n",
    "    return log_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "@_check_column_dtypes\n",
    "@lru_cache_df(maxsize=1)\n",
    "def merge_ranking_true_pred(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user,\n",
    "    col_item,\n",
    "    col_rating,\n",
    "    col_prediction,\n",
    "    relevancy_method,\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "\n",
    "\n",
    "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
    "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
    "    n_users = len(common_users)\n",
    "\n",
    "    if relevancy_method == \"top_k\":\n",
    "        top_k = k\n",
    "    elif relevancy_method == \"by_threshold\":\n",
    "        top_k = threshold\n",
    "    elif relevancy_method is None:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
    "    df_hit = get_top_k_items(\n",
    "        dataframe=rating_pred_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_prediction,\n",
    "        k=top_k,\n",
    "    )\n",
    "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
    "        [col_user, col_item, \"rank\"]\n",
    "    ]\n",
    "\n",
    "    df_hit_count = pd.merge(\n",
    "        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
    "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
    "            {\"actual\": \"count\"}\n",
    "        ),\n",
    "        on=col_user,\n",
    "    )\n",
    "\n",
    "    return df_hit, df_hit_count, n_users\n",
    "\n",
    "\n",
    "def precision_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "\n",
    "\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / k).sum() / n_users\n",
    "\n",
    "\n",
    "def recall_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users\n",
    "\n",
    "\n",
    "def ndcg_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "\n",
    "\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    df_dcg = df_hit.copy()\n",
    "\n",
    "    df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
    "\n",
    "    df_dcg = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "\n",
    "    df_ndcg = pd.merge(df_dcg, df_hit_count, on=[col_user])\n",
    "    df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
    "        lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n",
    "    )\n",
    "\n",
    "    return (df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users\n",
    "\n",
    "\n",
    "def map_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "\n",
    "\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    df_hit_sorted = df_hit.copy()\n",
    "    df_hit_sorted[\"rr\"] = (\n",
    "        df_hit_sorted.groupby(col_user).cumcount() + 1\n",
    "    ) / df_hit_sorted[\"rank\"]\n",
    "    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n",
    "\n",
    "    df_merge = pd.merge(df_hit_sorted, df_hit_count, on=col_user)\n",
    "    return (df_merge[\"rr\"] / df_merge[\"actual\"]).sum() / n_users\n",
    "\n",
    "\n",
    "def get_top_k_items(\n",
    "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
    "):\n",
    "\n",
    "    if k is None:\n",
    "        top_k_items = dataframe\n",
    "    else:\n",
    "        top_k_items = (\n",
    "            dataframe.groupby(col_user, as_index=False)\n",
    "            .apply(lambda x: x.nlargest(k, col_rating))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    rmse.__name__: rmse,\n",
    "    mae.__name__: mae,\n",
    "    rsquared.__name__: rsquared,\n",
    "    exp_var.__name__: exp_var,\n",
    "    precision_at_k.__name__: precision_at_k,\n",
    "    recall_at_k.__name__: recall_at_k,\n",
    "    ndcg_at_k.__name__: ndcg_at_k,\n",
    "    map_at_k.__name__: map_at_k,\n",
    "}\n",
    "\n",
    "\n",
    "def _check_column_dtypes_diversity_serendipity(func):\n",
    "\n",
    "    @wraps(func)\n",
    "    def check_column_dtypes_diversity_serendipity_wrapper(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        item_feature_df=None,\n",
    "        item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "        col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        col_sim=DEFAULT_SIMILARITY_COL,\n",
    "        col_relevance=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "\n",
    "        if not has_columns(train_df, [col_user, col_item]):\n",
    "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
    "        if not has_columns(reco_df, [col_user, col_item]):\n",
    "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
    "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
    "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
    "        if col_relevance is None:\n",
    "            col_relevance = DEFAULT_RELEVANCE_COL\n",
    "            # relevance term, default is 1 (relevant) for all\n",
    "            reco_df = reco_df[[col_user, col_item]]\n",
    "            reco_df[col_relevance] = 1.0\n",
    "        else:\n",
    "            col_relevance = col_relevance\n",
    "            reco_df = reco_df[[col_user, col_item, col_relevance]].astype(\n",
    "                {col_relevance: np.float16}\n",
    "            )\n",
    "        if item_sim_measure == \"item_feature_vector\":\n",
    "            required_columns = [col_item, col_item_features]\n",
    "            if item_feature_df is not None:\n",
    "                if not has_columns(item_feature_df, required_columns):\n",
    "                    raise ValueError(\"Missing columns in item_feature_df DataFrame\")\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"item_feature_df not specified! item_feature_df must be provided \"\n",
    "                    \"if choosing to use item_feature_vector to calculate item similarity. \"\n",
    "                    \"item_feature_df should have columns: \" + str(required_columns)\n",
    "                )\n",
    "        # check if reco_df contains any user_item pairs that are already shown in train_df\n",
    "        count_intersection = pd.merge(\n",
    "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
    "        ).shape[0]\n",
    "        if count_intersection != 0:\n",
    "            raise Exception(\n",
    "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
    "            )\n",
    "\n",
    "        return func(\n",
    "            train_df=train_df,\n",
    "            reco_df=reco_df,\n",
    "            item_feature_df=item_feature_df,\n",
    "            item_sim_measure=item_sim_measure,\n",
    "            col_user=col_user,\n",
    "            col_item=col_item,\n",
    "            col_sim=col_sim,\n",
    "            col_relevance=col_relevance,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return check_column_dtypes_diversity_serendipity_wrapper\n",
    "\n",
    "\n",
    "def _check_column_dtypes_novelty_coverage(func):\n",
    "\n",
    "    @wraps(func)\n",
    "    def check_column_dtypes_novelty_coverage_wrapper(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "\n",
    "        if not has_columns(train_df, [col_user, col_item]):\n",
    "            raise ValueError(\"Missing columns in train_df DataFrame\")\n",
    "        if not has_columns(reco_df, [col_user, col_item]):\n",
    "            raise ValueError(\"Missing columns in reco_df DataFrame\")\n",
    "        if not has_same_base_dtype(train_df, reco_df, columns=[col_user, col_item]):\n",
    "            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n",
    "\n",
    "        count_intersection = pd.merge(\n",
    "            train_df, reco_df, how=\"inner\", on=[col_user, col_item]\n",
    "        ).shape[0]\n",
    "        if count_intersection != 0:\n",
    "            raise Exception(\n",
    "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
    "            )\n",
    "\n",
    "        return func(\n",
    "            train_df=train_df,\n",
    "            reco_df=reco_df,\n",
    "            col_user=col_user,\n",
    "            col_item=col_item,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return check_column_dtypes_novelty_coverage_wrapper\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "def _get_pairwise_items(\n",
    "    df,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "):\n",
    "\n",
    "    df_user_i1 = df[[col_user, col_item]]\n",
    "    df_user_i1.columns = [col_user, \"i1\"]\n",
    "\n",
    "    df_user_i2 = df[[col_user, col_item]]\n",
    "    df_user_i2.columns = [col_user, \"i2\"]\n",
    "\n",
    "    df_user_i1_i2 = pd.merge(df_user_i1, df_user_i2, how=\"inner\", on=[col_user])\n",
    "\n",
    "    df_pairwise_items = df_user_i1_i2[(df_user_i1_i2[\"i1\"] <= df_user_i1_i2[\"i2\"])][\n",
    "        [col_user, \"i1\", \"i2\"]\n",
    "    ].reset_index(drop=True)\n",
    "    return df_pairwise_items\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "def _get_cosine_similarity(\n",
    "    train_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "):\n",
    "\n",
    "    if item_sim_measure == \"item_cooccurrence_count\":\n",
    "        df_cosine_similarity = _get_cooccurrence_similarity(\n",
    "            train_df, col_user, col_item, col_sim\n",
    "        )\n",
    "    elif item_sim_measure == \"item_feature_vector\":\n",
    "        df_cosine_similarity = _get_item_feature_similarity(\n",
    "            item_feature_df, col_item_features, col_user, col_item\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"item_sim_measure not recognized! The available options include 'item_cooccurrence_count' and 'item_feature_vector'.\"\n",
    "        )\n",
    "    return df_cosine_similarity\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "def _get_cooccurrence_similarity(\n",
    "    train_df,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "):\n",
    "\n",
    "    pairs = _get_pairwise_items(train_df, col_user, col_item)\n",
    "    pairs_count = pd.DataFrame(\n",
    "        {\"count\": pairs.groupby([\"i1\", \"i2\"]).size()}\n",
    "    ).reset_index()\n",
    "    item_count = pd.DataFrame(\n",
    "        {\"count\": train_df.groupby([col_item]).size()}\n",
    "    ).reset_index()\n",
    "    item_count[\"item_sqrt_count\"] = item_count[\"count\"] ** 0.5\n",
    "    item_co_occur = pairs_count.merge(\n",
    "        item_count[[col_item, \"item_sqrt_count\"]],\n",
    "        left_on=[\"i1\"],\n",
    "        right_on=[col_item],\n",
    "    ).drop(columns=[col_item])\n",
    "\n",
    "    item_co_occur.columns = [\"i1\", \"i2\", \"count\", \"i1_sqrt_count\"]\n",
    "\n",
    "    item_co_occur = item_co_occur.merge(\n",
    "        item_count[[col_item, \"item_sqrt_count\"]],\n",
    "        left_on=[\"i2\"],\n",
    "        right_on=[col_item],\n",
    "    ).drop(columns=[col_item])\n",
    "    item_co_occur.columns = [\n",
    "        \"i1\",\n",
    "        \"i2\",\n",
    "        \"count\",\n",
    "        \"i1_sqrt_count\",\n",
    "        \"i2_sqrt_count\",\n",
    "    ]\n",
    "\n",
    "    item_co_occur[col_sim] = item_co_occur[\"count\"] / (\n",
    "        item_co_occur[\"i1_sqrt_count\"] * item_co_occur[\"i2_sqrt_count\"]\n",
    "    )\n",
    "    df_cosine_similarity = (\n",
    "        item_co_occur[[\"i1\", \"i2\", col_sim]]\n",
    "        .sort_values([\"i1\", \"i2\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_cosine_similarity\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "def _get_item_feature_similarity(\n",
    "    item_feature_df,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "):\n",
    "\n",
    "    df1 = item_feature_df[[col_item, col_item_features]]\n",
    "    df1.columns = [\"i1\", \"f1\"]\n",
    "    df1[\"key\"] = 0\n",
    "    df2 = item_feature_df[[col_item, col_item_features]]\n",
    "    df2.columns = [\"i2\", \"f2\"]\n",
    "    df2[\"key\"] = 0\n",
    "\n",
    "    df = pd.merge(df1, df2, on=\"key\", how=\"outer\").drop(\"key\", axis=1)\n",
    "    df_item_feature_pair = df[(df[\"i1\"] <= df[\"i2\"])].reset_index(drop=True)\n",
    "\n",
    "    df_item_feature_pair[col_sim] = df_item_feature_pair.apply(\n",
    "        lambda x: float(x.f1.dot(x.f2))\n",
    "        / float(np.linalg.norm(x.f1, 2) * np.linalg.norm(x.f2, 2)),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_cosine_similarity = df_item_feature_pair[[\"i1\", \"i2\", col_sim]].sort_values(\n",
    "        [\"i1\", \"i2\"]\n",
    "    )\n",
    "\n",
    "    return df_cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "def _get_intralist_similarity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "):\n",
    "\n",
    "    pairs = _get_pairwise_items(reco_df, col_user, col_item)\n",
    "    similarity_df = _get_cosine_similarity(\n",
    "        train_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "    )\n",
    "\n",
    "    item_pair_sim = pairs.merge(similarity_df, on=[\"i1\", \"i2\"], how=\"left\")\n",
    "    item_pair_sim[col_sim].fillna(0, inplace=True)\n",
    "    item_pair_sim = item_pair_sim.loc[\n",
    "        item_pair_sim[\"i1\"] != item_pair_sim[\"i2\"]\n",
    "    ].reset_index(drop=True)\n",
    "    df_intralist_similarity = (\n",
    "        item_pair_sim.groupby([col_user]).agg({col_sim: \"mean\"}).reset_index()\n",
    "    )\n",
    "    df_intralist_similarity.columns = [col_user, \"avg_il_sim\"]\n",
    "\n",
    "    return df_intralist_similarity\n",
    "\n",
    "\n",
    "@_check_column_dtypes_diversity_serendipity\n",
    "@lru_cache_df(maxsize=1)\n",
    "def user_diversity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "    col_relevance=None,\n",
    "):\n",
    "\n",
    "\n",
    "    df_intralist_similarity = _get_intralist_similarity(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "    )\n",
    "    df_user_diversity = df_intralist_similarity\n",
    "    df_user_diversity[\"user_diversity\"] = 1 - df_user_diversity[\"avg_il_sim\"]\n",
    "    df_user_diversity = (\n",
    "        df_user_diversity[[col_user, \"user_diversity\"]]\n",
    "        .sort_values(col_user)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_user_diversity\n",
    "\n",
    "\n",
    "@_check_column_dtypes_diversity_serendipity\n",
    "def diversity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "    col_relevance=None,\n",
    "):\n",
    "\n",
    "    df_user_diversity = user_diversity(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "    )\n",
    "    avg_diversity = df_user_diversity.agg({\"user_diversity\": \"mean\"})[0]\n",
    "    return avg_diversity\n",
    "\n",
    "\n",
    "@_check_column_dtypes_novelty_coverage\n",
    "@lru_cache_df(maxsize=1)\n",
    "def historical_item_novelty(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "):\n",
    "\n",
    "\n",
    "    n_records = train_df.shape[0]\n",
    "    item_count = pd.DataFrame(\n",
    "        {\"count\": train_df.groupby([col_item]).size()}\n",
    "    ).reset_index()\n",
    "    item_count[\"item_novelty\"] = -np.log2(item_count[\"count\"] / n_records)\n",
    "    df_item_novelty = (\n",
    "        item_count[[col_item, \"item_novelty\"]]\n",
    "        .sort_values(col_item)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_item_novelty\n",
    "\n",
    "\n",
    "@_check_column_dtypes_novelty_coverage\n",
    "def novelty(train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL):\n",
    "\n",
    "    df_item_novelty = historical_item_novelty(train_df, reco_df, col_user, col_item)\n",
    "    n_recommendations = reco_df.shape[0]\n",
    "    reco_item_count = pd.DataFrame(\n",
    "        {\"count\": reco_df.groupby([col_item]).size()}\n",
    "    ).reset_index()\n",
    "    reco_item_novelty = reco_item_count.merge(df_item_novelty, on=col_item)\n",
    "    reco_item_novelty[\"product\"] = (\n",
    "        reco_item_novelty[\"count\"] * reco_item_novelty[\"item_novelty\"]\n",
    "    )\n",
    "    avg_novelty = reco_item_novelty.agg({\"product\": \"sum\"})[0] / n_recommendations\n",
    "\n",
    "    return avg_novelty\n",
    "\n",
    "\n",
    "@_check_column_dtypes_diversity_serendipity\n",
    "@lru_cache_df(maxsize=1)\n",
    "def user_item_serendipity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "    col_relevance=None,\n",
    "):\n",
    "\n",
    "    df_cosine_similarity = _get_cosine_similarity(\n",
    "        train_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "    )\n",
    "    reco_user_item = reco_df[[col_user, col_item]]\n",
    "    reco_user_item[\"reco_item_tmp\"] = reco_user_item[col_item]\n",
    "\n",
    "    train_user_item = train_df[[col_user, col_item]]\n",
    "    train_user_item.columns = [col_user, \"train_item_tmp\"]\n",
    "\n",
    "    reco_train_user_item = reco_user_item.merge(train_user_item, on=[col_user])\n",
    "    reco_train_user_item[\"i1\"] = reco_train_user_item[\n",
    "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
    "    ].min(axis=1)\n",
    "    reco_train_user_item[\"i2\"] = reco_train_user_item[\n",
    "        [\"reco_item_tmp\", \"train_item_tmp\"]\n",
    "    ].max(axis=1)\n",
    "\n",
    "    reco_train_user_item_sim = reco_train_user_item.merge(\n",
    "        df_cosine_similarity, on=[\"i1\", \"i2\"], how=\"left\"\n",
    "    )\n",
    "    reco_train_user_item_sim[col_sim].fillna(0, inplace=True)\n",
    "\n",
    "    reco_user_item_avg_sim = (\n",
    "        reco_train_user_item_sim.groupby([col_user, col_item])\n",
    "        .agg({col_sim: \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    reco_user_item_avg_sim.columns = [\n",
    "        col_user,\n",
    "        col_item,\n",
    "        \"avg_item2interactedHistory_sim\",\n",
    "    ]\n",
    "\n",
    "    df_user_item_serendipity = reco_user_item_avg_sim.merge(\n",
    "        reco_df, on=[col_user, col_item]\n",
    "    )\n",
    "    df_user_item_serendipity[\"user_item_serendipity\"] = (\n",
    "        1 - df_user_item_serendipity[\"avg_item2interactedHistory_sim\"]\n",
    "    ) * df_user_item_serendipity[col_relevance]\n",
    "    df_user_item_serendipity = (\n",
    "        df_user_item_serendipity[[col_user, col_item, \"user_item_serendipity\"]]\n",
    "        .sort_values([col_user, col_item])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_user_item_serendipity\n",
    "\n",
    "\n",
    "@lru_cache_df(maxsize=1)\n",
    "@_check_column_dtypes_diversity_serendipity\n",
    "def user_serendipity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "    col_relevance=None,\n",
    "):\n",
    "\n",
    "    df_user_item_serendipity = user_item_serendipity(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "        col_relevance,\n",
    "    )\n",
    "    df_user_serendipity = (\n",
    "        df_user_item_serendipity.groupby(col_user)\n",
    "        .agg({\"user_item_serendipity\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_user_serendipity.columns = [col_user, \"user_serendipity\"]\n",
    "    df_user_serendipity = df_user_serendipity.sort_values(col_user).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return df_user_serendipity\n",
    "\n",
    "\n",
    "@_check_column_dtypes_diversity_serendipity\n",
    "def serendipity(\n",
    "    train_df,\n",
    "    reco_df,\n",
    "    item_feature_df=None,\n",
    "    item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
    "    col_item_features=DEFAULT_ITEM_FEATURES_COL,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_sim=DEFAULT_SIMILARITY_COL,\n",
    "    col_relevance=None,\n",
    "):\n",
    "\n",
    "    df_user_serendipity = user_serendipity(\n",
    "        train_df,\n",
    "        reco_df,\n",
    "        item_feature_df,\n",
    "        item_sim_measure,\n",
    "        col_item_features,\n",
    "        col_user,\n",
    "        col_item,\n",
    "        col_sim,\n",
    "        col_relevance,\n",
    "    )\n",
    "    avg_serendipity = df_user_serendipity.agg({\"user_serendipity\": \"mean\"})[0]\n",
    "    return avg_serendipity\n",
    "\n",
    "\n",
    "@_check_column_dtypes_novelty_coverage\n",
    "def catalog_coverage(\n",
    "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
    "):\n",
    "\n",
    "\n",
    "    count_distinct_item_reco = reco_df[col_item].nunique()\n",
    "\n",
    "    count_distinct_item_train = train_df[col_item].nunique()\n",
    "\n",
    "    c_coverage = count_distinct_item_reco / count_distinct_item_train\n",
    "    return c_coverage\n",
    "\n",
    "\n",
    "@_check_column_dtypes_novelty_coverage\n",
    "def distributional_coverage(\n",
    "    train_df, reco_df, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL\n",
    "):\n",
    "\n",
    "    df_itemcnt_reco = pd.DataFrame(\n",
    "        {\"count\": reco_df.groupby([col_item]).size()}\n",
    "    ).reset_index()\n",
    "\n",
    "    count_row_reco = reco_df.shape[0]\n",
    "\n",
    "    df_entropy = df_itemcnt_reco\n",
    "    df_entropy[\"p(i)\"] = df_entropy[\"count\"] / count_row_reco\n",
    "    df_entropy[\"entropy(i)\"] = df_entropy[\"p(i)\"] * np.log2(df_entropy[\"p(i)\"])\n",
    "\n",
    "    d_coverage = -df_entropy.agg({\"entropy(i)\": \"sum\"})[0]\n",
    "\n",
    "    return d_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmQKDm_NduJ2"
   },
   "source": [
    "SAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "c7CUBSCEdE8h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy import sparse\n",
    "\n",
    "COOCCUR = \"cooccurrence\"\n",
    "JACCARD = \"jaccard\"\n",
    "LIFT = \"lift\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class SARSingleNode:\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        col_rating=DEFAULT_RATING_COL,\n",
    "        col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
    "        col_prediction=DEFAULT_PREDICTION_COL,\n",
    "        similarity_type=JACCARD,\n",
    "        time_decay_coefficient=30,\n",
    "        time_now=None,\n",
    "        timedecay_formula=False,\n",
    "        threshold=1,\n",
    "        normalize=False,\n",
    "    ):\n",
    "\n",
    "        self.col_rating = col_rating\n",
    "        self.col_item = col_item\n",
    "        self.col_user = col_user\n",
    "        self.col_timestamp = col_timestamp\n",
    "        self.col_prediction = col_prediction\n",
    "\n",
    "        if similarity_type not in [COOCCUR, JACCARD, LIFT]:\n",
    "            raise ValueError(\n",
    "                'Similarity type must be one of [\"cooccurrence\" | \"jaccard\" | \"lift\"]'\n",
    "            )\n",
    "        self.similarity_type = similarity_type\n",
    "        self.time_decay_half_life = (\n",
    "            time_decay_coefficient * 24 * 60 * 60\n",
    "        )\n",
    "        self.time_decay_flag = timedecay_formula\n",
    "        self.time_now = time_now\n",
    "        self.threshold = threshold\n",
    "        self.user_affinity = None\n",
    "        self.item_similarity = None\n",
    "        self.item_frequencies = None\n",
    "\n",
    "        if self.threshold <= 0:\n",
    "            raise ValueError(\"Threshold cannot be < 1\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.col_unity_rating = \"_unity_rating\"\n",
    "        self.unity_user_affinity = None\n",
    "\n",
    "        self.col_item_id = \"_indexed_items\"\n",
    "        self.col_user_id = \"_indexed_users\"\n",
    "\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "        self.rating_min = None\n",
    "        self.rating_max = None\n",
    "        self.user2index = None\n",
    "        self.item2index = None\n",
    "        self.index2item = None\n",
    "\n",
    "    def compute_affinity_matrix(self, df, rating_col):\n",
    "\n",
    "        return sparse.coo_matrix(\n",
    "            (df[rating_col], (df[self.col_user_id], df[self.col_item_id])),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        ).tocsr()\n",
    "\n",
    "    def compute_time_decay(self, df, decay_column):\n",
    "\n",
    "\n",
    "        if self.time_now is None:\n",
    "            self.time_now = df[self.col_timestamp].max()\n",
    "\n",
    "        df[decay_column] *= exponential_decay(\n",
    "            value=df[self.col_timestamp],\n",
    "            max_val=self.time_now,\n",
    "            half_life=self.time_decay_half_life,\n",
    "        )\n",
    "\n",
    "        return df.groupby([self.col_user, self.col_item]).sum().reset_index()\n",
    "\n",
    "    def compute_cooccurrence_matrix(self, df):\n",
    "\n",
    "        user_item_hits = sparse.coo_matrix(\n",
    "            (np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        ).tocsr()\n",
    "\n",
    "        item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n",
    "        item_cooccurrence = item_cooccurrence.multiply(\n",
    "            item_cooccurrence >= self.threshold\n",
    "        )\n",
    "\n",
    "        return item_cooccurrence.astype(df[self.col_rating].dtype)\n",
    "\n",
    "    def set_index(self, df):\n",
    "\n",
    "\n",
    "        self.index2item = dict(enumerate(df[self.col_item].unique()))\n",
    "\n",
    "        self.item2index = {v: k for k, v in self.index2item.items()}\n",
    "\n",
    "        self.user2index = {x[1]: x[0] for x in enumerate(df[self.col_user].unique())}\n",
    "\n",
    "        self.n_users = len(self.user2index)\n",
    "        self.n_items = len(self.index2item)\n",
    "\n",
    "    def fit(self, df):\n",
    "\n",
    "        if self.index2item is None:\n",
    "            self.set_index(df)\n",
    "\n",
    "        logger.info(\"Collecting user affinity matrix\")\n",
    "        if not np.issubdtype(df[self.col_rating].dtype, np.number):\n",
    "            raise TypeError(\"Rating column data type must be numeric\")\n",
    "\n",
    "        select_columns = [self.col_user, self.col_item, self.col_rating]\n",
    "        if self.time_decay_flag:\n",
    "            select_columns += [self.col_timestamp]\n",
    "        temp_df = df[select_columns].copy()\n",
    "\n",
    "        if self.time_decay_flag:\n",
    "            logger.info(\"Calculating time-decayed affinities\")\n",
    "            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n",
    "\n",
    "        logger.info(\"Creating index columns\")\n",
    "\n",
    "        temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(\n",
    "            lambda item: self.item2index.get(item, np.NaN)\n",
    "        )\n",
    "        temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(\n",
    "            lambda user: self.user2index.get(user, np.NaN)\n",
    "        )\n",
    "\n",
    "        if self.normalize:\n",
    "            self.rating_min = temp_df[self.col_rating].min()\n",
    "            self.rating_max = temp_df[self.col_rating].max()\n",
    "            logger.info(\"Calculating normalization factors\")\n",
    "            temp_df[self.col_unity_rating] = 1.0\n",
    "            if self.time_decay_flag:\n",
    "                temp_df = self.compute_time_decay(\n",
    "                    df=temp_df, decay_column=self.col_unity_rating\n",
    "                )\n",
    "            self.unity_user_affinity = self.compute_affinity_matrix(\n",
    "                df=temp_df, rating_col=self.col_unity_rating\n",
    "            )\n",
    "\n",
    "\n",
    "        logger.info(\"Building user affinity sparse matrix\")\n",
    "        self.user_affinity = self.compute_affinity_matrix(\n",
    "            df=temp_df, rating_col=self.col_rating\n",
    "        )\n",
    "\n",
    "\n",
    "        logger.info(\"Calculating item co-occurrence\")\n",
    "        item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n",
    "\n",
    "        del temp_df\n",
    "\n",
    "        self.item_frequencies = item_cooccurrence.diagonal()\n",
    "\n",
    "        logger.info(\"Calculating item similarity\")\n",
    "        if self.similarity_type == COOCCUR:\n",
    "            logger.info(\"Using co-occurrence based similarity\")\n",
    "            self.item_similarity = item_cooccurrence\n",
    "        elif self.similarity_type == JACCARD:\n",
    "            logger.info(\"Using jaccard based similarity\")\n",
    "            self.item_similarity = jaccard(item_cooccurrence).astype(\n",
    "                df[self.col_rating].dtype\n",
    "            )\n",
    "        elif self.similarity_type == LIFT:\n",
    "            logger.info(\"Using lift based similarity\")\n",
    "            self.item_similarity = lift(item_cooccurrence).astype(\n",
    "                df[self.col_rating].dtype\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown similarity type: {}\".format(self.similarity_type))\n",
    "\n",
    "        del item_cooccurrence\n",
    "\n",
    "        logger.info(\"Done training\")\n",
    "\n",
    "    def score(self, test, remove_seen=False):\n",
    "\n",
    "        user_ids = list(\n",
    "            map(\n",
    "                lambda user: self.user2index.get(user, np.NaN),\n",
    "                test[self.col_user].unique(),\n",
    "            )\n",
    "        )\n",
    "        if any(np.isnan(user_ids)):\n",
    "            raise ValueError(\"SAR cannot score users that are not in the training set\")\n",
    "\n",
    "        logger.info(\"Calculating recommendation scores\")\n",
    "        test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n",
    "\n",
    "        if isinstance(test_scores, sparse.spmatrix):\n",
    "            test_scores = test_scores.toarray()\n",
    "\n",
    "        if self.normalize:\n",
    "            counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n",
    "            user_min_scores = (\n",
    "                np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1])\n",
    "                * self.rating_min\n",
    "            )\n",
    "            user_max_scores = (\n",
    "                np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1])\n",
    "                * self.rating_max\n",
    "            )\n",
    "            test_scores = rescale(\n",
    "                test_scores,\n",
    "                self.rating_min,\n",
    "                self.rating_max,\n",
    "                user_min_scores,\n",
    "                user_max_scores,\n",
    "            )\n",
    "\n",
    "        if remove_seen:\n",
    "            logger.info(\"Removing seen items\")\n",
    "            test_scores += self.user_affinity[user_ids, :] * -np.inf\n",
    "\n",
    "        return test_scores\n",
    "\n",
    "    def get_popularity_based_topk(self, top_k=10, sort_top_k=True):\n",
    "\n",
    "\n",
    "        test_scores = np.array([self.item_frequencies])\n",
    "\n",
    "        logger.info(\"Getting top K\")\n",
    "        top_items, top_scores = get_top_k_scored_items(\n",
    "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
    "        )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
    "                self.col_prediction: top_scores.flatten(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n",
    "\n",
    "\n",
    "        item_ids = np.asarray(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda item: self.item2index.get(item, np.NaN),\n",
    "                    items[self.col_item].values,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.col_rating in items.columns:\n",
    "            ratings = items[self.col_rating]\n",
    "        else:\n",
    "            ratings = pd.Series(np.ones_like(item_ids))\n",
    "\n",
    "        if self.col_user in items.columns:\n",
    "            test_users = items[self.col_user]\n",
    "            user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n",
    "            user_ids = test_users.map(user2index)\n",
    "        else:\n",
    "\n",
    "            test_users = pd.Series(np.zeros_like(item_ids))\n",
    "            user_ids = test_users\n",
    "        n_users = user_ids.drop_duplicates().shape[0]\n",
    "\n",
    "        pseudo_affinity = sparse.coo_matrix(\n",
    "            (ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)\n",
    "        ).tocsr()\n",
    "\n",
    "        test_scores = pseudo_affinity.dot(self.item_similarity)\n",
    "\n",
    "        test_scores[user_ids, item_ids] = -np.inf\n",
    "\n",
    "        top_items, top_scores = get_top_k_scored_items(\n",
    "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                self.col_user: np.repeat(\n",
    "                    test_users.drop_duplicates().values, top_items.shape[1]\n",
    "                ),\n",
    "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
    "                self.col_prediction: top_scores.flatten(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return df.replace(-np.inf, np.nan).dropna()\n",
    "\n",
    "    def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n",
    "\n",
    "        test_scores = self.score(test, remove_seen=remove_seen)\n",
    "\n",
    "        top_items, top_scores = get_top_k_scored_items(\n",
    "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                self.col_user: np.repeat(\n",
    "                    test[self.col_user].drop_duplicates().values, top_items.shape[1]\n",
    "                ),\n",
    "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
    "                self.col_prediction: top_scores.flatten(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return df.replace(-np.inf, np.nan).dropna()\n",
    "\n",
    "    def predict(self, test):\n",
    "\n",
    "\n",
    "        test_scores = self.score(test)\n",
    "        user_ids = np.asarray(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda user: self.user2index.get(user, np.NaN),\n",
    "                    test[self.col_user].values,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        item_ids = np.asarray(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda item: self.item2index.get(item, np.NaN),\n",
    "                    test[self.col_item].values,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        nans = np.isnan(item_ids)\n",
    "        if any(nans):\n",
    "            logger.warning(\n",
    "                \"Items found in test not seen during training, new items will have score of 0\"\n",
    "            )\n",
    "            test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n",
    "            item_ids[nans] = self.n_items\n",
    "            item_ids = item_ids.astype(\"int64\")\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                self.col_user: test[self.col_user].values,\n",
    "                self.col_item: test[self.col_item].values,\n",
    "                self.col_prediction: test_scores[user_ids, item_ids],\n",
    "            }\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Dq1CqN2EeM-k"
   },
   "outputs": [],
   "source": [
    "\n",
    "TOP_K = 10\n",
    "\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "8PCi8Eeee3Go",
    "outputId": "4539fdfe-6f70-4996-b63a-85578a9276c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpshq_6hqn/ml-100k.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:01<00:00, 3.11kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d32e1f56-5678-416d-81af-82a549b0d862\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>875747190</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>883888671</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>879138235</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>876503793</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d32e1f56-5678-416d-81af-82a549b0d862')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d32e1f56-5678-416d-81af-82a549b0d862 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d32e1f56-5678-416d-81af-82a549b0d862');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   UserId  MovieId  Rating  Timestamp         Title\n",
       "0     196      242     3.0  881250949  Kolya (1996)\n",
       "1      63      242     3.0  875747190  Kolya (1996)\n",
       "2     226      242     5.0  883888671  Kolya (1996)\n",
       "3     154      242     3.0  879138235  Kolya (1996)\n",
       "4     306      242     5.0  876503793  Kolya (1996)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=['UserId', 'MovieId', 'Rating', 'Timestamp'],\n",
    "    title_col='Title'\n",
    ")\n",
    "\n",
    "data.loc[:, 'Rating'] = data['Rating'].astype(np.float32)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Eb-7qVBSkiMz"
   },
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"col_user\": \"UserId\",\n",
    "    \"col_item\": \"MovieId\",\n",
    "    \"col_rating\": \"Rating\",\n",
    "    \"col_timestamp\": \"Timestamp\",\n",
    "    \"col_prediction\": \"Prediction\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Nwc59oSAlvyj"
   },
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(data, ratio=0.75, col_user=header[\"col_user\"], col_item=header[\"col_item\"], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "DZyks1cgl0R2"
   },
   "outputs": [],
   "source": [
    "model = SARSingleNode(\n",
    "    similarity_type=\"jaccard\", \n",
    "    time_decay_coefficient=30, \n",
    "    time_now=None, \n",
    "    timedecay_formula=True, \n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "2Maj0eKKl7_l"
   },
   "outputs": [],
   "source": [
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zDx9ZTdkl-Nz"
   },
   "outputs": [],
   "source": [
    "top_k = model.recommend_k_items(test, remove_seen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "l87sAvb9oSiE",
    "outputId": "dd668043-114c-438c-f98b-9990aadb6b27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-40df7685-bc32-4dc7-ba54-bb689d7e15ea\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9420</th>\n",
       "      <td>943</td>\n",
       "      <td>82</td>\n",
       "      <td>21.313228</td>\n",
       "      <td>Jurassic Park (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>943</td>\n",
       "      <td>403</td>\n",
       "      <td>21.158839</td>\n",
       "      <td>Batman (1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9422</th>\n",
       "      <td>943</td>\n",
       "      <td>568</td>\n",
       "      <td>20.962922</td>\n",
       "      <td>Speed (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9423</th>\n",
       "      <td>943</td>\n",
       "      <td>423</td>\n",
       "      <td>20.162170</td>\n",
       "      <td>E.T. the Extra-Terrestrial (1982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9424</th>\n",
       "      <td>943</td>\n",
       "      <td>89</td>\n",
       "      <td>19.890513</td>\n",
       "      <td>Blade Runner (1982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9425</th>\n",
       "      <td>943</td>\n",
       "      <td>393</td>\n",
       "      <td>19.832944</td>\n",
       "      <td>Mrs. Doubtfire (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9426</th>\n",
       "      <td>943</td>\n",
       "      <td>11</td>\n",
       "      <td>19.570244</td>\n",
       "      <td>Seven (Se7en) (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>943</td>\n",
       "      <td>71</td>\n",
       "      <td>19.553877</td>\n",
       "      <td>Lion King, The (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>943</td>\n",
       "      <td>202</td>\n",
       "      <td>19.422129</td>\n",
       "      <td>Groundhog Day (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9429</th>\n",
       "      <td>943</td>\n",
       "      <td>238</td>\n",
       "      <td>19.115604</td>\n",
       "      <td>Raising Arizona (1987)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40df7685-bc32-4dc7-ba54-bb689d7e15ea')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-40df7685-bc32-4dc7-ba54-bb689d7e15ea button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-40df7685-bc32-4dc7-ba54-bb689d7e15ea');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      UserId  MovieId  Prediction                              Title\n",
       "9420     943       82   21.313228               Jurassic Park (1993)\n",
       "9421     943      403   21.158839                      Batman (1989)\n",
       "9422     943      568   20.962922                       Speed (1994)\n",
       "9423     943      423   20.162170  E.T. the Extra-Terrestrial (1982)\n",
       "9424     943       89   19.890513                Blade Runner (1982)\n",
       "9425     943      393   19.832944              Mrs. Doubtfire (1993)\n",
       "9426     943       11   19.570244               Seven (Se7en) (1995)\n",
       "9427     943       71   19.553877              Lion King, The (1994)\n",
       "9428     943      202   19.422129               Groundhog Day (1993)\n",
       "9429     943      238   19.115604             Raising Arizona (1987)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_k_with_titles = (top_k.join(data[['MovieId', 'Title']].drop_duplicates().set_index('MovieId'), \n",
    "                                on='MovieId', \n",
    "                                how='inner').sort_values(by=['UserId', 'Prediction'], ascending=False))\n",
    "display(top_k_with_titles.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04Q5wMi1u6p7"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hx9F4smEobua"
   },
   "outputs": [],
   "source": [
    "\n",
    "args = [test, top_k]\n",
    "kwargs = dict(col_user='UserId', \n",
    "              col_item='MovieId', \n",
    "              col_rating='Rating', \n",
    "              col_prediction='Prediction', \n",
    "              relevancy_method='top_k', \n",
    "              k=TOP_K)\n",
    "\n",
    "eval_map = map_at_k(*args, **kwargs)\n",
    "eval_ndcg = ndcg_at_k(*args, **kwargs)\n",
    "eval_precision = precision_at_k(*args, **kwargs)\n",
    "eval_recall = recall_at_k(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WCfRzBHvB3t",
    "outputId": "3f80b780-0b25-46c4-fac1-5dbe7100249f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Top K:\t\t 10\n",
      "MAP:\t\t 0.095544\n",
      "NDCG:\t\t 0.350232\n",
      "Precision@K:\t 0.305726\n",
      "Recall@K:\t 0.164690\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model:\",\n",
    "      f\"Top K:\\t\\t {TOP_K}\",\n",
    "      f\"MAP:\\t\\t {eval_map:f}\",\n",
    "      f\"NDCG:\\t\\t {eval_ndcg:f}\",\n",
    "      f\"Precision@K:\\t {eval_precision:f}\",\n",
    "      f\"Recall@K:\\t {eval_recall:f}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AJ0rE_kvDmK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SAR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
